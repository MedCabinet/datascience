{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "u50QLBDD1ORj"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "everyday_df = pd.read_csv('Cannabis_Strains_Features.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "Ny3VcyZn189Y",
        "outputId": "75eb9a62-80c5-4e47-85b0-07f4c8733773"
      },
      "source": [
        "everyday_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Strain    Type  Rating                                     Effects  \\\n",
              "0          100-Og  hybrid     4.0  Creative,Energetic,Tingly,Euphoric,Relaxed   \n",
              "1  98-White-Widow  hybrid     4.7    Relaxed,Aroused,Creative,Happy,Energetic   \n",
              "2            1024  sativa     4.4   Uplifted,Happy,Relaxed,Energetic,Creative   \n",
              "3        13-Dawgs  hybrid     4.2     Tingly,Creative,Hungry,Relaxed,Uplifted   \n",
              "4        24K-Gold  hybrid     4.6   Happy,Relaxed,Euphoric,Uplifted,Talkative   \n",
              "\n",
              "                      Flavor  \\\n",
              "0        Earthy,Sweet,Citrus   \n",
              "1      Flowery,Violet,Diesel   \n",
              "2    Spicy/Herbal,Sage,Woody   \n",
              "3  Apricot,Citrus,Grapefruit   \n",
              "4       Citrus,Earthy,Orange   \n",
              "\n",
              "                                         Description  \n",
              "0  $100 OG is a 50/50 hybrid strain that packs a ...  \n",
              "1  The ‘98 Aloha White Widow is an especially pot...  \n",
              "2  1024 is a sativa-dominant hybrid bred in Spain...  \n",
              "3  13 Dawgs is a hybrid of G13 and Chemdawg genet...  \n",
              "4  Also known as Kosher Tangie, 24k Gold is a 60%...  "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Strain</th>\n      <th>Type</th>\n      <th>Rating</th>\n      <th>Effects</th>\n      <th>Flavor</th>\n      <th>Description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100-Og</td>\n      <td>hybrid</td>\n      <td>4.0</td>\n      <td>Creative,Energetic,Tingly,Euphoric,Relaxed</td>\n      <td>Earthy,Sweet,Citrus</td>\n      <td>$100 OG is a 50/50 hybrid strain that packs a ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>98-White-Widow</td>\n      <td>hybrid</td>\n      <td>4.7</td>\n      <td>Relaxed,Aroused,Creative,Happy,Energetic</td>\n      <td>Flowery,Violet,Diesel</td>\n      <td>The ‘98 Aloha White Widow is an especially pot...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1024</td>\n      <td>sativa</td>\n      <td>4.4</td>\n      <td>Uplifted,Happy,Relaxed,Energetic,Creative</td>\n      <td>Spicy/Herbal,Sage,Woody</td>\n      <td>1024 is a sativa-dominant hybrid bred in Spain...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>13-Dawgs</td>\n      <td>hybrid</td>\n      <td>4.2</td>\n      <td>Tingly,Creative,Hungry,Relaxed,Uplifted</td>\n      <td>Apricot,Citrus,Grapefruit</td>\n      <td>13 Dawgs is a hybrid of G13 and Chemdawg genet...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24K-Gold</td>\n      <td>hybrid</td>\n      <td>4.6</td>\n      <td>Happy,Relaxed,Euphoric,Uplifted,Talkative</td>\n      <td>Citrus,Earthy,Orange</td>\n      <td>Also known as Kosher Tangie, 24k Gold is a 60%...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CoeluOIBHZ8",
        "outputId": "fee19e5b-47ed-452c-d5d7-e517cb4805da"
      },
      "source": [
        "# !python -m spacy download en_core_web_lg"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVzqT5vpEQRz"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-12-ea827c232f49>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"en_core_web_lg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\.virtualenvs\\datascience-ZCu60gyM\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\.virtualenvs\\datascience-ZCu60gyM\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87ptsrveNlPK"
      },
      "source": [
        "def tokenize(doc):\n",
        "  lemmas = []\n",
        "    \n",
        "  doc = nlp(doc)\n",
        "    \n",
        "  for token in doc: \n",
        "    if ((token.is_stop == False) and (token.is_punct == False) and (token.pos_ != 'PRON')):\n",
        "      lemmas.append(token.lemma_)\n",
        "  \n",
        "  return lemmas\n",
        "\n",
        "everyday_df['Description_2'] = everyday_df['Description'].apply(tokenize)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzyHHCWBPRAE",
        "outputId": "329548e2-e3c1-4d20-b7aa-9afcaadbdced"
      },
      "source": [
        "everyday_df['bow_col'] = \"string\"\n",
        "for i in range(0, len(everyday_df['Effects'])):\n",
        "  everyday_df['bow_col'][i] = everyday_df['Effects'][i].split(\",\") + everyday_df['Flavor'][i].split(\",\") + everyday_df['Description_2'][i]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3gbHPn-r8I4"
      },
      "source": [
        "everyday_df = everyday_df.drop(columns=[\"Effects\", \"Flavor\", \"Description\", \"Description_2\"], axis=1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "O6tK306usGnh",
        "outputId": "f2093175-608f-49db-a282-66e08c0d7a5a"
      },
      "source": [
        "everyday_df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Strain</th>\n",
              "      <th>Type</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Description_2</th>\n",
              "      <th>bow_col</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100-Og</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.0</td>\n",
              "      <td>[$, 100, OG, 50/50, hybrid, strain, pack, stro...</td>\n",
              "      <td>[Creative, Energetic, Tingly, Euphoric, Relaxe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>98-White-Widow</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.7</td>\n",
              "      <td>[98, Aloha, White, Widow, especially, potent, ...</td>\n",
              "      <td>[Relaxed, Aroused, Creative, Happy, Energetic,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1024</td>\n",
              "      <td>sativa</td>\n",
              "      <td>4.4</td>\n",
              "      <td>[1024, sativa, dominant, hybrid, breed, Spain,...</td>\n",
              "      <td>[Uplifted, Happy, Relaxed, Energetic, Creative...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13-Dawgs</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.2</td>\n",
              "      <td>[13, Dawgs, hybrid, G13, Chemdawg, genetic, br...</td>\n",
              "      <td>[Tingly, Creative, Hungry, Relaxed, Uplifted, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>24K-Gold</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.6</td>\n",
              "      <td>[know, Kosher, Tangie, 24k, gold, 60, indica, ...</td>\n",
              "      <td>[Happy, Relaxed, Euphoric, Uplifted, Talkative...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Strain  ...                                            bow_col\n",
              "0          100-Og  ...  [Creative, Energetic, Tingly, Euphoric, Relaxe...\n",
              "1  98-White-Widow  ...  [Relaxed, Aroused, Creative, Happy, Energetic,...\n",
              "2            1024  ...  [Uplifted, Happy, Relaxed, Energetic, Creative...\n",
              "3        13-Dawgs  ...  [Tingly, Creative, Hungry, Relaxed, Uplifted, ...\n",
              "4        24K-Gold  ...  [Happy, Relaxed, Euphoric, Uplifted, Talkative...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrvdOg4_P0N5",
        "outputId": "71b75780-72fb-438f-f7a5-f85c3defcc3b"
      },
      "source": [
        "everyday_df['bow_col'][0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Creative',\n",
              " 'Energetic',\n",
              " 'Tingly',\n",
              " 'Euphoric',\n",
              " 'Relaxed',\n",
              " 'Earthy',\n",
              " 'Sweet',\n",
              " 'Citrus',\n",
              " '$',\n",
              " '100',\n",
              " 'OG',\n",
              " '50/50',\n",
              " 'hybrid',\n",
              " 'strain',\n",
              " 'pack',\n",
              " 'strong',\n",
              " 'punch',\n",
              " 'supposedly',\n",
              " 'refer',\n",
              " 'strength',\n",
              " 'high',\n",
              " 'price',\n",
              " 'start',\n",
              " 'show',\n",
              " 'Hollywood',\n",
              " 'plant',\n",
              " '$',\n",
              " '100',\n",
              " 'OG',\n",
              " 'tend',\n",
              " 'produce',\n",
              " 'large',\n",
              " 'dark',\n",
              " 'green',\n",
              " 'bud',\n",
              " 'stem',\n",
              " 'user',\n",
              " 'report',\n",
              " 'strong',\n",
              " 'body',\n",
              " 'effect',\n",
              " 'indica',\n",
              " 'pain',\n",
              " 'relief',\n",
              " 'alert',\n",
              " 'cerebral',\n",
              " 'feeling',\n",
              " 'thank',\n",
              " 'sativa']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuHrKuszQ6Zx",
        "outputId": "e9ac057f-4d76-4311-9af0-ea11915a5f6e"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "word_counts = Counter()\n",
        "everyday_df['bow_col'].apply(lambda x: word_counts.update(x))\n",
        "word_counts.most_common(10)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('strain', 3021),\n",
              " ('  ', 2138),\n",
              " ('Happy', 1902),\n",
              " ('Relaxed', 1758),\n",
              " ('Euphoric', 1669),\n",
              " ('effect', 1596),\n",
              " ('Uplifted', 1535),\n",
              " ('indica', 1399),\n",
              " ('hybrid', 1312),\n",
              " ('Sweet', 1155)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "OC2PRHNBQkt-",
        "outputId": "069583d4-2162-4baf-a89c-6225a61115f7"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vect = TfidfVectorizer(stop_words=nlp.Defaults.stop_words,                                  max_features=1000)\n",
        "vect.fit(everyday_df['bow_col'].astype('str'))\n",
        "dtm = vect.transform(everyday_df['bow_col'].astype('str'))\n",
        "dtm = pd.DataFrame(dtm.todense(),columns=vect.get_feature_names())\n",
        "dtm.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>18</th>\n",
              "      <th>1st</th>\n",
              "      <th>20</th>\n",
              "      <th>2012</th>\n",
              "      <th>2014</th>\n",
              "      <th>2015</th>\n",
              "      <th>2016</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>2nd</th>\n",
              "      <th>30</th>\n",
              "      <th>3rd</th>\n",
              "      <th>40</th>\n",
              "      <th>47</th>\n",
              "      <th>50</th>\n",
              "      <th>55</th>\n",
              "      <th>60</th>\n",
              "      <th>65</th>\n",
              "      <th>70</th>\n",
              "      <th>75</th>\n",
              "      <th>80</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>abate</th>\n",
              "      <th>ability</th>\n",
              "      <th>accent</th>\n",
              "      <th>ace</th>\n",
              "      <th>ache</th>\n",
              "      <th>achieve</th>\n",
              "      <th>act</th>\n",
              "      <th>active</th>\n",
              "      <th>activity</th>\n",
              "      <th>...</th>\n",
              "      <th>useful</th>\n",
              "      <th>user</th>\n",
              "      <th>usher</th>\n",
              "      <th>usually</th>\n",
              "      <th>utilize</th>\n",
              "      <th>valley</th>\n",
              "      <th>vanilla</th>\n",
              "      <th>variation</th>\n",
              "      <th>variety</th>\n",
              "      <th>version</th>\n",
              "      <th>veteran</th>\n",
              "      <th>vibrant</th>\n",
              "      <th>vigorous</th>\n",
              "      <th>violet</th>\n",
              "      <th>wait</th>\n",
              "      <th>want</th>\n",
              "      <th>warm</th>\n",
              "      <th>washington</th>\n",
              "      <th>way</th>\n",
              "      <th>week</th>\n",
              "      <th>weigh</th>\n",
              "      <th>weight</th>\n",
              "      <th>weighted</th>\n",
              "      <th>west</th>\n",
              "      <th>white</th>\n",
              "      <th>wide</th>\n",
              "      <th>widow</th>\n",
              "      <th>win</th>\n",
              "      <th>winner</th>\n",
              "      <th>wonder</th>\n",
              "      <th>woody</th>\n",
              "      <th>work</th>\n",
              "      <th>world</th>\n",
              "      <th>worth</th>\n",
              "      <th>wrap</th>\n",
              "      <th>wreck</th>\n",
              "      <th>xa0</th>\n",
              "      <th>year</th>\n",
              "      <th>yield</th>\n",
              "      <th>zesty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.347529</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.173267</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.122812</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.269103</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.151957</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.353412</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.450098</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.117527</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.620051</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.115618</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.154098</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.151959</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.116128</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.121769</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    10   11   12        13   14   15  ...  wrap  wreck  xa0  year  yield  zesty\n",
              "0  0.0  0.0  0.0  0.000000  0.0  0.0  ...   0.0    0.0  0.0   0.0    0.0    0.0\n",
              "1  0.0  0.0  0.0  0.000000  0.0  0.0  ...   0.0    0.0  0.0   0.0    0.0    0.0\n",
              "2  0.0  0.0  0.0  0.000000  0.0  0.0  ...   0.0    0.0  0.0   0.0    0.0    0.0\n",
              "3  0.0  0.0  0.0  0.620051  0.0  0.0  ...   0.0    0.0  0.0   0.0    0.0    0.0\n",
              "4  0.0  0.0  0.0  0.000000  0.0  0.0  ...   0.0    0.0  0.0   0.0    0.0    0.0\n",
              "\n",
              "[5 rows x 1000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7sQJJ0zR3Ey",
        "outputId": "d708e40c-851b-4094-ab10-023642efd76b"
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "nn = NearestNeighbors(n_neighbors=10, algorithm='kd_tree')\n",
        "nn.fit(dtm)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NearestNeighbors(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n",
              "                 metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
              "                 radius=1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gDSvLDovRN2"
      },
      "source": [
        "random_pot = [\"I want something to take the edge off of my day, really mellow me out and help get me in the zone\"]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U2ZiOk3R89z"
      },
      "source": [
        "new = vect.transform(random_pot)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK2MlRBpvmNx",
        "outputId": "c54dcab2-2d5d-48f6-88c5-8c0006eeac77"
      },
      "source": [
        "nn.kneighbors(new.todense())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1.        , 1.        , 1.        , 1.21833403, 1.22405446,\n",
              "         1.25974847, 1.27575554, 1.27670641, 1.28326677, 1.29198692]]),\n",
              " array([[1652, 1653, 1651, 2318, 1785,  672, 2251,  166, 2068, 1124]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5x-DdjwVwmZf",
        "outputId": "ab8c27f2-66d7-4b68-e068-a644448515b7"
      },
      "source": [
        "everyday_df['bow_col'][2068]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Relaxed',\n",
              " 'Focused',\n",
              " 'Uplifted',\n",
              " 'Talkative',\n",
              " 'Happy',\n",
              " 'Pine',\n",
              " 'Earthy',\n",
              " 'Spicy/Herbal',\n",
              " 'Suzy',\n",
              " 'Q',\n",
              " 'high',\n",
              " 'CBD',\n",
              " 'low',\n",
              " 'thc',\n",
              " 'strain',\n",
              " 'piney',\n",
              " 'taste',\n",
              " 'help',\n",
              " 'treat',\n",
              " 'symptom',\n",
              " 'little',\n",
              " 'high',\n",
              " 'great',\n",
              " 'strain',\n",
              " 'add',\n",
              " 'burgeon',\n",
              " 'world',\n",
              " 'CBD',\n",
              " 'strain',\n",
              " 'strain',\n",
              " 'test',\n",
              " 'upwards',\n",
              " '59:1',\n",
              " 'CBD',\n",
              " 'THC',\n",
              " 'hybrid',\n",
              " 'great',\n",
              " 'daytime',\n",
              " 'use',\n",
              " 'want',\n",
              " 'relieve',\n",
              " 'chronic',\n",
              " 'pain',\n",
              " 'nausea',\n",
              " 'arthritis',\n",
              " 'muscle',\n",
              " 'spasm',\n",
              " 'anxiety',\n",
              " 'psychoactive',\n",
              " 'effect',\n",
              " '\\xa0 \\xa0\\xa0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQawZ0X61nNO",
        "outputId": "984fa3a2-6548-45d0-ed6c-8cd883fd54a2"
      },
      "source": [
        "everyday_df['Rating'].value_counts()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.5    323\n",
              "4.3    299\n",
              "4.4    287\n",
              "4.6    245\n",
              "4.2    227\n",
              "5.0    219\n",
              "4.7    166\n",
              "4.8    162\n",
              "4.0    109\n",
              "4.1    101\n",
              "4.9     61\n",
              "0.0     35\n",
              "3.9     29\n",
              "3.8     23\n",
              "3.7     16\n",
              "3.0     13\n",
              "3.6     11\n",
              "3.5     10\n",
              "3.4      5\n",
              "3.2      3\n",
              "3.3      3\n",
              "3.1      1\n",
              "2.5      1\n",
              "1.0      1\n",
              "2.8      1\n",
              "Name: Rating, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo_LhDrYxoD4",
        "outputId": "e536440b-7af8-4343-e76f-7d06981f388c"
      },
      "source": [
        "import numpy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "def create_model(units=32):\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Dense(units, activation=\"relu\", input_dim=1000))\n",
        "  \n",
        "  n_nodes_output = 25\n",
        "  model.add(Dense(n_nodes_output, activation='softmax'))\n",
        "  \n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "model_class = KerasClassifier(build_fn=create_model, verbose=1)\n",
        "\n",
        "param_grid = {'batch_size': [4, 8],\n",
        "              'epochs': [5, 6, 9],\n",
        "              'units':[24, 32]}\n",
        "\n",
        "grid = GridSearchCV(estimator=model_class, \n",
        "                    param_grid=param_grid, \n",
        "                    n_jobs=1, \n",
        "                    verbose=1)\n",
        "\n",
        "grid_result = grid.fit(dtm[:1000], everyday_df['Rating'][:1000])\n",
        "\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9708 - accuracy: 0.1200\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5122 - accuracy: 0.2075\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3798 - accuracy: 0.2663\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2611 - accuracy: 0.3825\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1238 - accuracy: 0.4238\n",
            "50/50 [==============================] - 0s 958us/step - loss: 2.5141 - accuracy: 0.2150\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9751 - accuracy: 0.1000\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5056 - accuracy: 0.1637\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3745 - accuracy: 0.3250\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2460 - accuracy: 0.4100\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1042 - accuracy: 0.4850\n",
            "50/50 [==============================] - 0s 798us/step - loss: 2.6114 - accuracy: 0.1500\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9632 - accuracy: 0.1138\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5427 - accuracy: 0.1600\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4194 - accuracy: 0.3075\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3037 - accuracy: 0.3850\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1724 - accuracy: 0.4588\n",
            "50/50 [==============================] - 0s 897us/step - loss: 2.3539 - accuracy: 0.2300\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 3.0106 - accuracy: 0.1075\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5561 - accuracy: 0.1538\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4155 - accuracy: 0.2375\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2952 - accuracy: 0.3375\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1521 - accuracy: 0.4050\n",
            "50/50 [==============================] - 0s 796us/step - loss: 2.5260 - accuracy: 0.1600\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9857 - accuracy: 0.1287\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5469 - accuracy: 0.1875\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4119 - accuracy: 0.2725\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2865 - accuracy: 0.3462\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1448 - accuracy: 0.4450\n",
            "50/50 [==============================] - 0s 791us/step - loss: 2.3942 - accuracy: 0.1950\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9558 - accuracy: 0.1100\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4975 - accuracy: 0.1875\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3614 - accuracy: 0.2887\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2124 - accuracy: 0.4225\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0337 - accuracy: 0.4900\n",
            "50/50 [==============================] - 0s 840us/step - loss: 2.5024 - accuracy: 0.2050\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9434 - accuracy: 0.1287\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4898 - accuracy: 0.2325\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3473 - accuracy: 0.3262\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1955 - accuracy: 0.4363\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0214 - accuracy: 0.4875\n",
            "50/50 [==============================] - 0s 801us/step - loss: 2.5821 - accuracy: 0.1600\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9442 - accuracy: 0.1138\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5317 - accuracy: 0.2113\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3961 - accuracy: 0.3250\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2481 - accuracy: 0.3537\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0806 - accuracy: 0.4462\n",
            "50/50 [==============================] - 0s 818us/step - loss: 2.3004 - accuracy: 0.2550\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9583 - accuracy: 0.1088\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4980 - accuracy: 0.2125\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3454 - accuracy: 0.3225\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1876 - accuracy: 0.3825\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0074 - accuracy: 0.4450\n",
            "50/50 [==============================] - 0s 823us/step - loss: 2.5122 - accuracy: 0.1650\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9683 - accuracy: 0.1112\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5215 - accuracy: 0.1825\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3699 - accuracy: 0.3175\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2087 - accuracy: 0.3900\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0322 - accuracy: 0.4550\n",
            "50/50 [==============================] - 0s 853us/step - loss: 2.3949 - accuracy: 0.2000\n",
            "Epoch 1/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9870 - accuracy: 0.1037\n",
            "Epoch 2/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5330 - accuracy: 0.1587\n",
            "Epoch 3/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3987 - accuracy: 0.2313\n",
            "Epoch 4/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2786 - accuracy: 0.3400\n",
            "Epoch 5/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1350 - accuracy: 0.4437\n",
            "Epoch 6/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.9795 - accuracy: 0.4762\n",
            "50/50 [==============================] - 0s 996us/step - loss: 2.5243 - accuracy: 0.2350\n",
            "Epoch 1/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 3.0441 - accuracy: 0.0962\n",
            "Epoch 2/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5196 - accuracy: 0.1562\n",
            "Epoch 3/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3522 - accuracy: 0.3288\n",
            "Epoch 4/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2216 - accuracy: 0.4238\n",
            "Epoch 5/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0775 - accuracy: 0.4737\n",
            "Epoch 6/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.9273 - accuracy: 0.5325\n",
            "50/50 [==============================] - 0s 842us/step - loss: 2.5914 - accuracy: 0.1600\n",
            "Epoch 1/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9940 - accuracy: 0.1138\n",
            "Epoch 2/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5538 - accuracy: 0.2050\n",
            "Epoch 3/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4180 - accuracy: 0.3137\n",
            "Epoch 4/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3068 - accuracy: 0.3512\n",
            "Epoch 5/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1792 - accuracy: 0.3950\n",
            "Epoch 6/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0428 - accuracy: 0.4412\n",
            "50/50 [==============================] - 0s 871us/step - loss: 2.2998 - accuracy: 0.2750\n",
            "Epoch 1/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9910 - accuracy: 0.1125\n",
            "Epoch 2/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5371 - accuracy: 0.1688\n",
            "Epoch 3/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4097 - accuracy: 0.3125\n",
            "Epoch 4/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2941 - accuracy: 0.3650\n",
            "Epoch 5/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1524 - accuracy: 0.4100\n",
            "Epoch 6/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.9946 - accuracy: 0.4988\n",
            "50/50 [==============================] - 0s 824us/step - loss: 2.5324 - accuracy: 0.1850\n",
            "Epoch 1/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9942 - accuracy: 0.1100\n",
            "Epoch 2/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5622 - accuracy: 0.1700\n",
            "Epoch 3/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4377 - accuracy: 0.2325\n",
            "Epoch 4/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3185 - accuracy: 0.3700\n",
            "Epoch 5/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1720 - accuracy: 0.4400\n",
            "Epoch 6/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0119 - accuracy: 0.5088\n",
            " 1/50 [..............................] - ETA: 0s - loss: 2.7674 - accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_test_batch_end` time: 0.0026s). Check your callbacks.\n",
            "50/50 [==============================] - 0s 885us/step - loss: 2.3921 - accuracy: 0.1800\n",
            "Epoch 1/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 3.0037 - accuracy: 0.0900\n",
            "Epoch 2/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5205 - accuracy: 0.1825\n",
            "Epoch 3/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3688 - accuracy: 0.2612\n",
            "Epoch 4/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2164 - accuracy: 0.3762\n",
            "Epoch 5/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0323 - accuracy: 0.4888\n",
            "Epoch 6/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.8317 - accuracy: 0.5575\n",
            "50/50 [==============================] - 0s 908us/step - loss: 2.5225 - accuracy: 0.2200\n",
            "Epoch 1/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9344 - accuracy: 0.1063\n",
            "Epoch 2/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4719 - accuracy: 0.2713\n",
            "Epoch 3/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3293 - accuracy: 0.3212\n",
            "Epoch 4/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1731 - accuracy: 0.4400\n",
            "Epoch 5/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.9910 - accuracy: 0.5088\n",
            "Epoch 6/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.7986 - accuracy: 0.5950\n",
            "50/50 [==============================] - 0s 943us/step - loss: 2.5587 - accuracy: 0.1850\n",
            "Epoch 1/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9589 - accuracy: 0.1200\n",
            "Epoch 2/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5292 - accuracy: 0.1750\n",
            "Epoch 3/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3942 - accuracy: 0.3288\n",
            "Epoch 4/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2491 - accuracy: 0.3925\n",
            "Epoch 5/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0817 - accuracy: 0.4688\n",
            "Epoch 6/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.9038 - accuracy: 0.5350\n",
            "50/50 [==============================] - 0s 879us/step - loss: 2.3136 - accuracy: 0.2300\n",
            "Epoch 1/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9436 - accuracy: 0.1163\n",
            "Epoch 2/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5015 - accuracy: 0.1663\n",
            "Epoch 3/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3708 - accuracy: 0.2887\n",
            "Epoch 4/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2228 - accuracy: 0.4050\n",
            "Epoch 5/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0475 - accuracy: 0.4988\n",
            "Epoch 6/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.8587 - accuracy: 0.5550\n",
            "50/50 [==============================] - 0s 899us/step - loss: 2.4841 - accuracy: 0.2050\n",
            "Epoch 1/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9430 - accuracy: 0.1138\n",
            "Epoch 2/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5078 - accuracy: 0.2000\n",
            "Epoch 3/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3824 - accuracy: 0.2663\n",
            "Epoch 4/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2418 - accuracy: 0.3675\n",
            "Epoch 5/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0824 - accuracy: 0.4350\n",
            "Epoch 6/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.9147 - accuracy: 0.4850\n",
            "50/50 [==============================] - 0s 957us/step - loss: 2.4049 - accuracy: 0.2100\n",
            "Epoch 1/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9979 - accuracy: 0.1075\n",
            "Epoch 2/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5213 - accuracy: 0.1937\n",
            "Epoch 3/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3764 - accuracy: 0.3250\n",
            "Epoch 4/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2465 - accuracy: 0.4175\n",
            "Epoch 5/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0974 - accuracy: 0.4450\n",
            "Epoch 6/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.9374 - accuracy: 0.5225\n",
            "Epoch 7/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.7772 - accuracy: 0.5725\n",
            "Epoch 8/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.6137 - accuracy: 0.6425\n",
            "Epoch 9/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.4568 - accuracy: 0.7038\n",
            "50/50 [==============================] - 0s 828us/step - loss: 2.5570 - accuracy: 0.2250\n",
            "Epoch 1/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9954 - accuracy: 0.1275\n",
            "Epoch 2/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5388 - accuracy: 0.1475\n",
            "Epoch 3/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3872 - accuracy: 0.2675\n",
            "Epoch 4/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2589 - accuracy: 0.4025\n",
            "Epoch 5/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1138 - accuracy: 0.4613\n",
            "Epoch 6/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.9514 - accuracy: 0.5450\n",
            "Epoch 7/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.7836 - accuracy: 0.6000\n",
            "Epoch 8/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.6161 - accuracy: 0.6475\n",
            "Epoch 9/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.4523 - accuracy: 0.6837\n",
            "50/50 [==============================] - 0s 830us/step - loss: 2.7164 - accuracy: 0.1400\n",
            "Epoch 1/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9650 - accuracy: 0.1112\n",
            "Epoch 2/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5362 - accuracy: 0.1762\n",
            "Epoch 3/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4088 - accuracy: 0.2700\n",
            "Epoch 4/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2928 - accuracy: 0.3575\n",
            "Epoch 5/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1691 - accuracy: 0.3875\n",
            "Epoch 6/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0323 - accuracy: 0.4563\n",
            "Epoch 7/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.8891 - accuracy: 0.4988\n",
            "Epoch 8/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.7413 - accuracy: 0.5962\n",
            "Epoch 9/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.5905 - accuracy: 0.6550\n",
            "50/50 [==============================] - 0s 826us/step - loss: 2.2993 - accuracy: 0.2250\n",
            "Epoch 1/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 3.0119 - accuracy: 0.1262\n",
            "Epoch 2/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5275 - accuracy: 0.1525\n",
            "Epoch 3/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3842 - accuracy: 0.3075\n",
            "Epoch 4/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2633 - accuracy: 0.3663\n",
            "Epoch 5/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1229 - accuracy: 0.4375\n",
            "Epoch 6/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.9763 - accuracy: 0.4812\n",
            "Epoch 7/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.8222 - accuracy: 0.5512\n",
            "Epoch 8/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.6733 - accuracy: 0.6150\n",
            "Epoch 9/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.5294 - accuracy: 0.6538\n",
            "50/50 [==============================] - 0s 893us/step - loss: 2.5409 - accuracy: 0.1700\n",
            "Epoch 1/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9994 - accuracy: 0.1088\n",
            "Epoch 2/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5353 - accuracy: 0.2275\n",
            "Epoch 3/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3971 - accuracy: 0.3275\n",
            "Epoch 4/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2797 - accuracy: 0.3850\n",
            "Epoch 5/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1526 - accuracy: 0.4062\n",
            "Epoch 6/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0161 - accuracy: 0.4700\n",
            "Epoch 7/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.8709 - accuracy: 0.5350\n",
            "Epoch 8/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.7311 - accuracy: 0.5838\n",
            "Epoch 9/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.5894 - accuracy: 0.6175\n",
            "50/50 [==============================] - 0s 834us/step - loss: 2.3984 - accuracy: 0.2000\n",
            "Epoch 1/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9349 - accuracy: 0.1088\n",
            "Epoch 2/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4831 - accuracy: 0.1963\n",
            "Epoch 3/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3413 - accuracy: 0.2962\n",
            "Epoch 4/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1947 - accuracy: 0.4288\n",
            "Epoch 5/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0183 - accuracy: 0.4638\n",
            "Epoch 6/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.8331 - accuracy: 0.5125\n",
            "Epoch 7/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.6435 - accuracy: 0.6050\n",
            "Epoch 8/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.4594 - accuracy: 0.7000\n",
            "Epoch 9/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.2781 - accuracy: 0.7475\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2.5994 - accuracy: 0.1850\n",
            "Epoch 1/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.8984 - accuracy: 0.1213\n",
            "Epoch 2/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4540 - accuracy: 0.2525\n",
            "Epoch 3/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3166 - accuracy: 0.3562\n",
            "Epoch 4/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1635 - accuracy: 0.4750\n",
            "Epoch 5/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.9948 - accuracy: 0.4963\n",
            "Epoch 6/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.8082 - accuracy: 0.5813\n",
            "Epoch 7/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.6262 - accuracy: 0.6388\n",
            "Epoch 8/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.4435 - accuracy: 0.6888\n",
            "Epoch 9/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.2743 - accuracy: 0.7462\n",
            "50/50 [==============================] - 0s 849us/step - loss: 2.6381 - accuracy: 0.1300\n",
            "Epoch 1/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9689 - accuracy: 0.1187\n",
            "Epoch 2/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5404 - accuracy: 0.1825\n",
            "Epoch 3/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3976 - accuracy: 0.2500\n",
            "Epoch 4/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2608 - accuracy: 0.3713\n",
            "Epoch 5/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1001 - accuracy: 0.4737\n",
            "Epoch 6/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.9248 - accuracy: 0.5500\n",
            "Epoch 7/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.7428 - accuracy: 0.6125\n",
            "Epoch 8/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.5608 - accuracy: 0.6825\n",
            "Epoch 9/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.3827 - accuracy: 0.7350\n",
            "50/50 [==============================] - 0s 986us/step - loss: 2.3502 - accuracy: 0.1950\n",
            "Epoch 1/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9749 - accuracy: 0.1325\n",
            "Epoch 2/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5266 - accuracy: 0.1488\n",
            "Epoch 3/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3840 - accuracy: 0.2438\n",
            "Epoch 4/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2290 - accuracy: 0.3738\n",
            "Epoch 5/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0449 - accuracy: 0.4263\n",
            "Epoch 6/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.8524 - accuracy: 0.5325\n",
            "Epoch 7/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.6561 - accuracy: 0.6025\n",
            "Epoch 8/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.4662 - accuracy: 0.6712\n",
            "Epoch 9/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.2846 - accuracy: 0.7525\n",
            "50/50 [==============================] - 0s 892us/step - loss: 2.5287 - accuracy: 0.1550\n",
            "Epoch 1/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9871 - accuracy: 0.1025\n",
            "Epoch 2/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5272 - accuracy: 0.1787\n",
            "Epoch 3/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3912 - accuracy: 0.2750\n",
            "Epoch 4/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2509 - accuracy: 0.3988\n",
            "Epoch 5/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0847 - accuracy: 0.4663\n",
            "Epoch 6/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.9042 - accuracy: 0.5400\n",
            "Epoch 7/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.7138 - accuracy: 0.5713\n",
            "Epoch 8/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.5246 - accuracy: 0.6650\n",
            "Epoch 9/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.3442 - accuracy: 0.7163\n",
            "50/50 [==============================] - 0s 970us/step - loss: 2.4142 - accuracy: 0.2000\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1129 - accuracy: 0.0862\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7636 - accuracy: 0.1550\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.5031 - accuracy: 0.2713\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3882 - accuracy: 0.3125\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2894 - accuracy: 0.3862\n",
            "25/25 [==============================] - 0s 982us/step - loss: 2.5273 - accuracy: 0.2150\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1433 - accuracy: 0.1013\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7671 - accuracy: 0.1975\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4820 - accuracy: 0.2062\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3876 - accuracy: 0.3525\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3135 - accuracy: 0.4100\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.6348 - accuracy: 0.1600\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0884 - accuracy: 0.1150\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6955 - accuracy: 0.1975\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.5144 - accuracy: 0.2663\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4202 - accuracy: 0.3175\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3280 - accuracy: 0.3537\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.3756 - accuracy: 0.2550\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0955 - accuracy: 0.1300\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6964 - accuracy: 0.1412\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4856 - accuracy: 0.2275\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3867 - accuracy: 0.2663\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2952 - accuracy: 0.3262\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5595 - accuracy: 0.1700\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1205 - accuracy: 0.1100\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7330 - accuracy: 0.1800\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.5126 - accuracy: 0.1875\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4198 - accuracy: 0.2925\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3294 - accuracy: 0.3625\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.4602 - accuracy: 0.1800\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0698 - accuracy: 0.0975\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6518 - accuracy: 0.1538\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4695 - accuracy: 0.1900\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3556 - accuracy: 0.2775\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2346 - accuracy: 0.4288\n",
            "25/25 [==============================] - 0s 959us/step - loss: 2.5207 - accuracy: 0.2200\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0905 - accuracy: 0.1100\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6233 - accuracy: 0.2125\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4124 - accuracy: 0.3063\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3049 - accuracy: 0.3875\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1948 - accuracy: 0.4288\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.6087 - accuracy: 0.1700\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1227 - accuracy: 0.0975\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7314 - accuracy: 0.2438\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4913 - accuracy: 0.3250\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3701 - accuracy: 0.3425\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2546 - accuracy: 0.3862\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.3582 - accuracy: 0.2350\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1022 - accuracy: 0.0938\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6672 - accuracy: 0.2000\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4353 - accuracy: 0.2450\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3198 - accuracy: 0.3338\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2025 - accuracy: 0.4025\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5565 - accuracy: 0.1400\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0898 - accuracy: 0.1050\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6585 - accuracy: 0.1988\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4538 - accuracy: 0.2800\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3435 - accuracy: 0.3675\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2328 - accuracy: 0.4162\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.4269 - accuracy: 0.1950\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1160 - accuracy: 0.1125\n",
            "Epoch 2/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7366 - accuracy: 0.1587\n",
            "Epoch 3/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4942 - accuracy: 0.2062\n",
            "Epoch 4/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3923 - accuracy: 0.2387\n",
            "Epoch 5/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3003 - accuracy: 0.3363\n",
            "Epoch 6/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1992 - accuracy: 0.3775\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5380 - accuracy: 0.2200\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0862 - accuracy: 0.1138\n",
            "Epoch 2/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6791 - accuracy: 0.1437\n",
            "Epoch 3/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4686 - accuracy: 0.1575\n",
            "Epoch 4/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3733 - accuracy: 0.3313\n",
            "Epoch 5/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2843 - accuracy: 0.3938\n",
            "Epoch 6/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1840 - accuracy: 0.4450\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.6249 - accuracy: 0.1700\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1370 - accuracy: 0.0962\n",
            "Epoch 2/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7966 - accuracy: 0.1775\n",
            "Epoch 3/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.5364 - accuracy: 0.1762\n",
            "Epoch 4/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4305 - accuracy: 0.2937\n",
            "Epoch 5/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3408 - accuracy: 0.3550\n",
            "Epoch 6/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2433 - accuracy: 0.4025\n",
            "25/25 [==============================] - 0s 960us/step - loss: 2.3501 - accuracy: 0.2400\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1138 - accuracy: 0.1112\n",
            "Epoch 2/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7136 - accuracy: 0.1375\n",
            "Epoch 3/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4820 - accuracy: 0.2175\n",
            "Epoch 4/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3860 - accuracy: 0.3013\n",
            "Epoch 5/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2953 - accuracy: 0.3438\n",
            "Epoch 6/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1944 - accuracy: 0.3913\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5535 - accuracy: 0.1350\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1151 - accuracy: 0.0938\n",
            "Epoch 2/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7142 - accuracy: 0.1375\n",
            "Epoch 3/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4855 - accuracy: 0.2163\n",
            "Epoch 4/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3855 - accuracy: 0.3325\n",
            "Epoch 5/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2916 - accuracy: 0.3462\n",
            "Epoch 6/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1911 - accuracy: 0.4238\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.4099 - accuracy: 0.2200\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1215 - accuracy: 0.1013\n",
            "Epoch 2/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7150 - accuracy: 0.1850\n",
            "Epoch 3/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4603 - accuracy: 0.1750\n",
            "Epoch 4/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3266 - accuracy: 0.3288\n",
            "Epoch 5/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1995 - accuracy: 0.3438\n",
            "Epoch 6/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.0688 - accuracy: 0.4837\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5226 - accuracy: 0.2100\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1291 - accuracy: 0.1050\n",
            "Epoch 2/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7138 - accuracy: 0.1300\n",
            "Epoch 3/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4523 - accuracy: 0.2362\n",
            "Epoch 4/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3391 - accuracy: 0.3738\n",
            "Epoch 5/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2272 - accuracy: 0.4038\n",
            "Epoch 6/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.0995 - accuracy: 0.4737\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.6103 - accuracy: 0.1900\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1262 - accuracy: 0.0900\n",
            "Epoch 2/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7446 - accuracy: 0.2025\n",
            "Epoch 3/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.5028 - accuracy: 0.2600\n",
            "Epoch 4/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3843 - accuracy: 0.3325\n",
            "Epoch 5/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2659 - accuracy: 0.4313\n",
            "Epoch 6/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1414 - accuracy: 0.4750\n",
            "25/25 [==============================] - 0s 952us/step - loss: 2.3277 - accuracy: 0.2500\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0667 - accuracy: 0.1125\n",
            "Epoch 2/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6242 - accuracy: 0.1312\n",
            "Epoch 3/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4338 - accuracy: 0.2512\n",
            "Epoch 4/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3248 - accuracy: 0.3550\n",
            "Epoch 5/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2141 - accuracy: 0.4275\n",
            "Epoch 6/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.0908 - accuracy: 0.4288\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5366 - accuracy: 0.1950\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1092 - accuracy: 0.1100\n",
            "Epoch 2/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6980 - accuracy: 0.1925\n",
            "Epoch 3/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4674 - accuracy: 0.2775\n",
            "Epoch 4/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3537 - accuracy: 0.3500\n",
            "Epoch 5/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2398 - accuracy: 0.3875\n",
            "Epoch 6/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1143 - accuracy: 0.4600\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.4019 - accuracy: 0.2150\n",
            "Epoch 1/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1113 - accuracy: 0.1112\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7281 - accuracy: 0.1713\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4796 - accuracy: 0.1975\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3645 - accuracy: 0.3150\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2641 - accuracy: 0.3913\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1576 - accuracy: 0.4487\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.0484 - accuracy: 0.4725\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.9365 - accuracy: 0.5063\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.8241 - accuracy: 0.5700\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5237 - accuracy: 0.2350\n",
            "Epoch 1/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1230 - accuracy: 0.1050\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7071 - accuracy: 0.1475\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4743 - accuracy: 0.2113\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3782 - accuracy: 0.3388\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2908 - accuracy: 0.3800\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1935 - accuracy: 0.4575\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.0902 - accuracy: 0.5125\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.9776 - accuracy: 0.5512\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.8631 - accuracy: 0.5825\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5961 - accuracy: 0.1650\n",
            "Epoch 1/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1352 - accuracy: 0.0975\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7818 - accuracy: 0.1612\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.5234 - accuracy: 0.2225\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4179 - accuracy: 0.3187\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3228 - accuracy: 0.3750\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2230 - accuracy: 0.4025\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1160 - accuracy: 0.4313\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.0102 - accuracy: 0.4787\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.8988 - accuracy: 0.5362\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.2839 - accuracy: 0.2400\n",
            "Epoch 1/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0869 - accuracy: 0.1213\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6809 - accuracy: 0.2237\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4855 - accuracy: 0.2400\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3941 - accuracy: 0.2887\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3003 - accuracy: 0.3775\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1928 - accuracy: 0.4450\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.0785 - accuracy: 0.4900\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.9625 - accuracy: 0.5375\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.8441 - accuracy: 0.5663\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5191 - accuracy: 0.1800\n",
            "Epoch 1/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1103 - accuracy: 0.1200\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7352 - accuracy: 0.1238\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.5178 - accuracy: 0.1775\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4258 - accuracy: 0.2912\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3443 - accuracy: 0.3425\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2493 - accuracy: 0.3775\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1469 - accuracy: 0.4137\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.0312 - accuracy: 0.4825\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.9162 - accuracy: 0.5175\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.4091 - accuracy: 0.1900\n",
            "Epoch 1/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0722 - accuracy: 0.1187\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 2.6184 - accuracy: 0.2387\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 2.4216 - accuracy: 0.3275\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3190 - accuracy: 0.3562\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2103 - accuracy: 0.4250\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 2.0909 - accuracy: 0.4863\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 1.9643 - accuracy: 0.5013\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 1.8352 - accuracy: 0.5487\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.7054 - accuracy: 0.5800\n",
            "25/25 [==============================] - 0s 2ms/step - loss: 2.5355 - accuracy: 0.2150\n",
            "Epoch 1/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1115 - accuracy: 0.1187\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 2.6835 - accuracy: 0.1488\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4350 - accuracy: 0.2788\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 2.3070 - accuracy: 0.4137\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 2.1843 - accuracy: 0.4487\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.0540 - accuracy: 0.4850\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 1.9160 - accuracy: 0.5288\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.7766 - accuracy: 0.5600\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.6404 - accuracy: 0.6350\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5975 - accuracy: 0.1650\n",
            "Epoch 1/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0965 - accuracy: 0.1013\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7069 - accuracy: 0.1500\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 2.4924 - accuracy: 0.2400\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 2.3704 - accuracy: 0.3225\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2497 - accuracy: 0.3950\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 2.1242 - accuracy: 0.4150\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.9933 - accuracy: 0.4963\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.8610 - accuracy: 0.5487\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.7261 - accuracy: 0.6087\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.3043 - accuracy: 0.2400\n",
            "Epoch 1/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1260 - accuracy: 0.1063\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6965 - accuracy: 0.1500\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 0s 2ms/step - loss: 2.4610 - accuracy: 0.1900\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3537 - accuracy: 0.3375\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2457 - accuracy: 0.3900\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1244 - accuracy: 0.4663\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.9950 - accuracy: 0.5113\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.8548 - accuracy: 0.5800\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.7168 - accuracy: 0.6112\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5212 - accuracy: 0.1600\n",
            "Epoch 1/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1256 - accuracy: 0.1187\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7139 - accuracy: 0.1475\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4676 - accuracy: 0.2275\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3543 - accuracy: 0.3787\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2401 - accuracy: 0.4137\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1154 - accuracy: 0.4650\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.9865 - accuracy: 0.5025\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.8499 - accuracy: 0.5437\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.7185 - accuracy: 0.5987\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.3829 - accuracy: 0.1900\n",
            "Epoch 1/6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  2.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "125/125 [==============================] - 0s 1ms/step - loss: 3.0327 - accuracy: 0.1080\n",
            "Epoch 2/6\n",
            "125/125 [==============================] - 0s 1ms/step - loss: 2.5751 - accuracy: 0.1410\n",
            "Epoch 3/6\n",
            "125/125 [==============================] - 0s 1ms/step - loss: 2.4194 - accuracy: 0.2990\n",
            "Epoch 4/6\n",
            "125/125 [==============================] - 0s 1ms/step - loss: 2.3047 - accuracy: 0.4020\n",
            "Epoch 5/6\n",
            "125/125 [==============================] - 0s 1ms/step - loss: 2.1827 - accuracy: 0.4580\n",
            "Epoch 6/6\n",
            "125/125 [==============================] - 0s 1ms/step - loss: 2.0479 - accuracy: 0.5010\n",
            "Best: 0.21199999749660492 using {'batch_size': 8, 'epochs': 6, 'units': 32}\n",
            "Means: 0.19000000059604644, Stdev: 0.030822070594994304 with: {'batch_size': 4, 'epochs': 5, 'units': 24}\n",
            "Means: 0.1969999998807907, Stdev: 0.034146740052801744 with: {'batch_size': 4, 'epochs': 5, 'units': 32}\n",
            "Means: 0.2070000022649765, Stdev: 0.042023804259663525 with: {'batch_size': 4, 'epochs': 6, 'units': 24}\n",
            "Means: 0.20999999940395356, Stdev: 0.015165751163218525 with: {'batch_size': 4, 'epochs': 6, 'units': 32}\n",
            "Means: 0.19199999868869783, Stdev: 0.03295451143693536 with: {'batch_size': 4, 'epochs': 9, 'units': 24}\n",
            "Means: 0.1729999989271164, Stdev: 0.026570661529608824 with: {'batch_size': 4, 'epochs': 9, 'units': 32}\n",
            "Means: 0.19600000083446503, Stdev: 0.03484250137076219 with: {'batch_size': 8, 'epochs': 5, 'units': 24}\n",
            "Means: 0.19199999868869783, Stdev: 0.034146741274682556 with: {'batch_size': 8, 'epochs': 5, 'units': 32}\n",
            "Means: 0.1969999998807907, Stdev: 0.03867815576585886 with: {'batch_size': 8, 'epochs': 6, 'units': 24}\n",
            "Means: 0.21199999749660492, Stdev: 0.021118713955991367 with: {'batch_size': 8, 'epochs': 6, 'units': 32}\n",
            "Means: 0.2020000010728836, Stdev: 0.03009982993404253 with: {'batch_size': 8, 'epochs': 9, 'units': 24}\n",
            "Means: 0.19399999976158142, Stdev: 0.030232431389685323 with: {'batch_size': 8, 'epochs': 9, 'units': 32}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5jnUb1GDglS",
        "outputId": "cfc6b2a8-0a67-4a6e-a267-a9b9dce977b1"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "grid_result.best_estimator_.model.save('eli_model.tf')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: eli_model.tf/assets\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}