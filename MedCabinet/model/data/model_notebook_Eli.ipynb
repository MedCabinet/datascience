{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "u50QLBDD1ORj"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "everyday_df = pd.read_csv('/content/Cannabis_Strains_Features.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny3VcyZn189Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "5660bc55-7a16-4c01-b590-d9bc42e5a159"
      },
      "source": [
        "everyday_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Strain</th>\n",
              "      <th>Type</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Effects</th>\n",
              "      <th>Flavor</th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100-Og</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Creative,Energetic,Tingly,Euphoric,Relaxed</td>\n",
              "      <td>Earthy,Sweet,Citrus</td>\n",
              "      <td>$100 OG is a 50/50 hybrid strain that packs a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>98-White-Widow</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.7</td>\n",
              "      <td>Relaxed,Aroused,Creative,Happy,Energetic</td>\n",
              "      <td>Flowery,Violet,Diesel</td>\n",
              "      <td>The ‘98 Aloha White Widow is an especially pot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1024</td>\n",
              "      <td>sativa</td>\n",
              "      <td>4.4</td>\n",
              "      <td>Uplifted,Happy,Relaxed,Energetic,Creative</td>\n",
              "      <td>Spicy/Herbal,Sage,Woody</td>\n",
              "      <td>1024 is a sativa-dominant hybrid bred in Spain...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13-Dawgs</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.2</td>\n",
              "      <td>Tingly,Creative,Hungry,Relaxed,Uplifted</td>\n",
              "      <td>Apricot,Citrus,Grapefruit</td>\n",
              "      <td>13 Dawgs is a hybrid of G13 and Chemdawg genet...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>24K-Gold</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.6</td>\n",
              "      <td>Happy,Relaxed,Euphoric,Uplifted,Talkative</td>\n",
              "      <td>Citrus,Earthy,Orange</td>\n",
              "      <td>Also known as Kosher Tangie, 24k Gold is a 60%...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Strain  ...                                        Description\n",
              "0          100-Og  ...  $100 OG is a 50/50 hybrid strain that packs a ...\n",
              "1  98-White-Widow  ...  The ‘98 Aloha White Widow is an especially pot...\n",
              "2            1024  ...  1024 is a sativa-dominant hybrid bred in Spain...\n",
              "3        13-Dawgs  ...  13 Dawgs is a hybrid of G13 and Chemdawg genet...\n",
              "4        24K-Gold  ...  Also known as Kosher Tangie, 24k Gold is a 60%...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CoeluOIBHZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7afc6b2a-2238-4623-c4fd-d6816372b1eb"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_lg==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz#egg=en_core_web_lg==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVzqT5vpEQRz"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87ptsrveNlPK"
      },
      "source": [
        "def tokenize(doc):\n",
        "  lemmas = []\n",
        "    \n",
        "  doc = nlp(doc)\n",
        "    \n",
        "  for token in doc: \n",
        "    if ((token.is_stop == False) and (token.is_punct == False) and (token.pos_ != 'PRON')):\n",
        "      lemmas.append(token.lemma_)\n",
        "  \n",
        "  return lemmas\n",
        "\n",
        "everyday_df['Description_2'] = everyday_df['Description'].apply(tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzyHHCWBPRAE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5ff9aa5-1a12-465b-d99c-c7b9ca805254"
      },
      "source": [
        "everyday_df['bow_col'] = \"string\"\n",
        "for i in range(0, len(everyday_df['Effects'])):\n",
        "  everyday_df['bow_col'][i] = everyday_df['Effects'][i].split(\",\") + everyday_df['Flavor'][i].split(\",\") + everyday_df['Description_2'][i]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giVphhXkPkOK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "7e4f9323-e0d1-4bc9-b28e-cb7a8a8a11ff"
      },
      "source": [
        "everyday_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Strain</th>\n",
              "      <th>Type</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Effects</th>\n",
              "      <th>Flavor</th>\n",
              "      <th>Description</th>\n",
              "      <th>Description_2</th>\n",
              "      <th>bow_col</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100-Og</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Creative,Energetic,Tingly,Euphoric,Relaxed</td>\n",
              "      <td>Earthy,Sweet,Citrus</td>\n",
              "      <td>$100 OG is a 50/50 hybrid strain that packs a ...</td>\n",
              "      <td>[$, 100, OG, 50/50, hybrid, strain, pack, stro...</td>\n",
              "      <td>[Creative, Energetic, Tingly, Euphoric, Relaxe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>98-White-Widow</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.7</td>\n",
              "      <td>Relaxed,Aroused,Creative,Happy,Energetic</td>\n",
              "      <td>Flowery,Violet,Diesel</td>\n",
              "      <td>The ‘98 Aloha White Widow is an especially pot...</td>\n",
              "      <td>[98, Aloha, White, Widow, especially, potent, ...</td>\n",
              "      <td>[Relaxed, Aroused, Creative, Happy, Energetic,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1024</td>\n",
              "      <td>sativa</td>\n",
              "      <td>4.4</td>\n",
              "      <td>Uplifted,Happy,Relaxed,Energetic,Creative</td>\n",
              "      <td>Spicy/Herbal,Sage,Woody</td>\n",
              "      <td>1024 is a sativa-dominant hybrid bred in Spain...</td>\n",
              "      <td>[1024, sativa, dominant, hybrid, breed, Spain,...</td>\n",
              "      <td>[Uplifted, Happy, Relaxed, Energetic, Creative...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13-Dawgs</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.2</td>\n",
              "      <td>Tingly,Creative,Hungry,Relaxed,Uplifted</td>\n",
              "      <td>Apricot,Citrus,Grapefruit</td>\n",
              "      <td>13 Dawgs is a hybrid of G13 and Chemdawg genet...</td>\n",
              "      <td>[13, Dawgs, hybrid, G13, Chemdawg, genetic, br...</td>\n",
              "      <td>[Tingly, Creative, Hungry, Relaxed, Uplifted, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>24K-Gold</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.6</td>\n",
              "      <td>Happy,Relaxed,Euphoric,Uplifted,Talkative</td>\n",
              "      <td>Citrus,Earthy,Orange</td>\n",
              "      <td>Also known as Kosher Tangie, 24k Gold is a 60%...</td>\n",
              "      <td>[know, Kosher, Tangie, 24k, gold, 60, indica, ...</td>\n",
              "      <td>[Happy, Relaxed, Euphoric, Uplifted, Talkative...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Strain  ...                                            bow_col\n",
              "0          100-Og  ...  [Creative, Energetic, Tingly, Euphoric, Relaxe...\n",
              "1  98-White-Widow  ...  [Relaxed, Aroused, Creative, Happy, Energetic,...\n",
              "2            1024  ...  [Uplifted, Happy, Relaxed, Energetic, Creative...\n",
              "3        13-Dawgs  ...  [Tingly, Creative, Hungry, Relaxed, Uplifted, ...\n",
              "4        24K-Gold  ...  [Happy, Relaxed, Euphoric, Uplifted, Talkative...\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrvdOg4_P0N5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d8f40c9-50e1-4669-bc1a-38e0573a9289"
      },
      "source": [
        "everyday_df['bow_col'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Creative',\n",
              " 'Energetic',\n",
              " 'Tingly',\n",
              " 'Euphoric',\n",
              " 'Relaxed',\n",
              " 'Earthy',\n",
              " 'Sweet',\n",
              " 'Citrus',\n",
              " '$',\n",
              " '100',\n",
              " 'OG',\n",
              " '50/50',\n",
              " 'hybrid',\n",
              " 'strain',\n",
              " 'pack',\n",
              " 'strong',\n",
              " 'punch',\n",
              " 'supposedly',\n",
              " 'refer',\n",
              " 'strength',\n",
              " 'high',\n",
              " 'price',\n",
              " 'start',\n",
              " 'show',\n",
              " 'Hollywood',\n",
              " 'plant',\n",
              " '$',\n",
              " '100',\n",
              " 'OG',\n",
              " 'tend',\n",
              " 'produce',\n",
              " 'large',\n",
              " 'dark',\n",
              " 'green',\n",
              " 'bud',\n",
              " 'stem',\n",
              " 'user',\n",
              " 'report',\n",
              " 'strong',\n",
              " 'body',\n",
              " 'effect',\n",
              " 'indica',\n",
              " 'pain',\n",
              " 'relief',\n",
              " 'alert',\n",
              " 'cerebral',\n",
              " 'feeling',\n",
              " 'thank',\n",
              " 'sativa']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuHrKuszQ6Zx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e89cd738-9b25-49fc-d6ae-48e567ae06f7"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "word_counts = Counter()\n",
        "everyday_df['bow_col'].apply(lambda x: word_counts.update(x))\n",
        "word_counts.most_common(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('strain', 3021),\n",
              " ('  ', 2138),\n",
              " ('Happy', 1902),\n",
              " ('Relaxed', 1758),\n",
              " ('Euphoric', 1669),\n",
              " ('effect', 1596),\n",
              " ('Uplifted', 1535),\n",
              " ('indica', 1399),\n",
              " ('hybrid', 1312),\n",
              " ('Sweet', 1155)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC2PRHNBQkt-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "19f60b9d-f908-436e-87a3-34c4309bbdf4"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vect = TfidfVectorizer(stop_words=nlp.Defaults.stop_words, max_features=1000)\n",
        "vect.fit(everyday_df['bow_col'].astype('str'))\n",
        "dtm = vect.transform(everyday_df['bow_col'].astype('str'))\n",
        "dtm = pd.DataFrame(dtm.todense(),columns=vect.get_feature_names())\n",
        "dtm.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>18</th>\n",
              "      <th>1st</th>\n",
              "      <th>20</th>\n",
              "      <th>2012</th>\n",
              "      <th>2014</th>\n",
              "      <th>2015</th>\n",
              "      <th>2016</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>2nd</th>\n",
              "      <th>30</th>\n",
              "      <th>3rd</th>\n",
              "      <th>40</th>\n",
              "      <th>47</th>\n",
              "      <th>50</th>\n",
              "      <th>55</th>\n",
              "      <th>60</th>\n",
              "      <th>65</th>\n",
              "      <th>70</th>\n",
              "      <th>75</th>\n",
              "      <th>80</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>abate</th>\n",
              "      <th>ability</th>\n",
              "      <th>accent</th>\n",
              "      <th>ace</th>\n",
              "      <th>ache</th>\n",
              "      <th>achieve</th>\n",
              "      <th>act</th>\n",
              "      <th>active</th>\n",
              "      <th>activity</th>\n",
              "      <th>...</th>\n",
              "      <th>useful</th>\n",
              "      <th>user</th>\n",
              "      <th>usher</th>\n",
              "      <th>usually</th>\n",
              "      <th>utilize</th>\n",
              "      <th>valley</th>\n",
              "      <th>vanilla</th>\n",
              "      <th>variation</th>\n",
              "      <th>variety</th>\n",
              "      <th>version</th>\n",
              "      <th>veteran</th>\n",
              "      <th>vibrant</th>\n",
              "      <th>vigorous</th>\n",
              "      <th>violet</th>\n",
              "      <th>wait</th>\n",
              "      <th>want</th>\n",
              "      <th>warm</th>\n",
              "      <th>washington</th>\n",
              "      <th>way</th>\n",
              "      <th>week</th>\n",
              "      <th>weigh</th>\n",
              "      <th>weight</th>\n",
              "      <th>weighted</th>\n",
              "      <th>west</th>\n",
              "      <th>white</th>\n",
              "      <th>wide</th>\n",
              "      <th>widow</th>\n",
              "      <th>win</th>\n",
              "      <th>winner</th>\n",
              "      <th>wonder</th>\n",
              "      <th>woody</th>\n",
              "      <th>work</th>\n",
              "      <th>world</th>\n",
              "      <th>worth</th>\n",
              "      <th>wrap</th>\n",
              "      <th>wreck</th>\n",
              "      <th>xa0</th>\n",
              "      <th>year</th>\n",
              "      <th>yield</th>\n",
              "      <th>zesty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.347529</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.173267</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.122812</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.269103</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.151957</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.353412</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.450098</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.117527</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.620051</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.115618</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.154098</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.151959</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.116128</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.121769</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    10   11   12        13   14   15  ...  wrap  wreck  xa0  year  yield  zesty\n",
              "0  0.0  0.0  0.0  0.000000  0.0  0.0  ...   0.0    0.0  0.0   0.0    0.0    0.0\n",
              "1  0.0  0.0  0.0  0.000000  0.0  0.0  ...   0.0    0.0  0.0   0.0    0.0    0.0\n",
              "2  0.0  0.0  0.0  0.000000  0.0  0.0  ...   0.0    0.0  0.0   0.0    0.0    0.0\n",
              "3  0.0  0.0  0.0  0.620051  0.0  0.0  ...   0.0    0.0  0.0   0.0    0.0    0.0\n",
              "4  0.0  0.0  0.0  0.000000  0.0  0.0  ...   0.0    0.0  0.0   0.0    0.0    0.0\n",
              "\n",
              "[5 rows x 1000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7sQJJ0zR3Ey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4a1b1e2-5a4d-45bd-d902-31f68162f31a"
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "nn = NearestNeighbors(n_neighbors=10, algorithm='kd_tree')\n",
        "nn.fit(dtm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NearestNeighbors(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n",
              "                 metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
              "                 radius=1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gDSvLDovRN2"
      },
      "source": [
        "random_pot = [\"I want something to take the edge off of my day, really mellow me out and help get me in the zone\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U2ZiOk3R89z"
      },
      "source": [
        "new = vect.transform(random_pot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK2MlRBpvmNx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7ded892-22f7-4a0a-97da-1355c8888e1f"
      },
      "source": [
        "nn.kneighbors(new.todense())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1.        , 1.        , 1.        , 1.21833403, 1.22405446,\n",
              "         1.25974847, 1.27575554, 1.27670641, 1.28326677, 1.29198692]]),\n",
              " array([[1652, 1653, 1651, 2318, 1785,  672, 2251,  166, 2068, 1124]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x-DdjwVwmZf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95721985-010e-4c6f-e2a4-73c7c67720fe"
      },
      "source": [
        "everyday_df['bow_col'][2068]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Relaxed',\n",
              " 'Focused',\n",
              " 'Uplifted',\n",
              " 'Talkative',\n",
              " 'Happy',\n",
              " 'Pine',\n",
              " 'Earthy',\n",
              " 'Spicy/Herbal',\n",
              " 'Suzy',\n",
              " 'Q',\n",
              " 'high',\n",
              " 'CBD',\n",
              " 'low',\n",
              " 'thc',\n",
              " 'strain',\n",
              " 'piney',\n",
              " 'taste',\n",
              " 'help',\n",
              " 'treat',\n",
              " 'symptom',\n",
              " 'little',\n",
              " 'high',\n",
              " 'great',\n",
              " 'strain',\n",
              " 'add',\n",
              " 'burgeon',\n",
              " 'world',\n",
              " 'CBD',\n",
              " 'strain',\n",
              " 'strain',\n",
              " 'test',\n",
              " 'upwards',\n",
              " '59:1',\n",
              " 'CBD',\n",
              " 'THC',\n",
              " 'hybrid',\n",
              " 'great',\n",
              " 'daytime',\n",
              " 'use',\n",
              " 'want',\n",
              " 'relieve',\n",
              " 'chronic',\n",
              " 'pain',\n",
              " 'nausea',\n",
              " 'arthritis',\n",
              " 'muscle',\n",
              " 'spasm',\n",
              " 'anxiety',\n",
              " 'psychoactive',\n",
              " 'effect',\n",
              " '\\xa0 \\xa0\\xa0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQawZ0X61nNO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd602919-c1cb-4c19-8929-877261ff7f43"
      },
      "source": [
        "everyday_df['Rating'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.5    323\n",
              "4.3    299\n",
              "4.4    287\n",
              "4.6    245\n",
              "4.2    227\n",
              "5.0    219\n",
              "4.7    166\n",
              "4.8    162\n",
              "4.0    109\n",
              "4.1    101\n",
              "4.9     61\n",
              "0.0     35\n",
              "3.9     29\n",
              "3.8     23\n",
              "3.7     16\n",
              "3.0     13\n",
              "3.6     11\n",
              "3.5     10\n",
              "3.4      5\n",
              "3.2      3\n",
              "3.3      3\n",
              "3.1      1\n",
              "2.5      1\n",
              "1.0      1\n",
              "2.8      1\n",
              "Name: Rating, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qo_LhDrYxoD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "466874f9-8675-4fba-d5cf-a4593b907efa"
      },
      "source": [
        "import numpy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "n_nodes = 32\n",
        "model.add(Dense(n_nodes, activation=\"relu\", input_dim=1000))\n",
        "\n",
        "n_nodes_output = 25\n",
        "model.add(Dense(n_nodes_output, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "results = model.fit(dtm[:1000], \n",
        "                    everyday_df['Rating'][:1000], \n",
        "                    epochs=7, \n",
        "                    validation_data=(dtm[1000:2000], everyday_df['Rating'][1000:2000]))\n",
        "\n",
        "#model_class = KerasClassifier(build_fn=model, verbose=1)\n",
        "\n",
        "#param_grid = {'batch_size': [32,64,512],\n",
        "#              'epochs': [3],\n",
        "#              'units':[32]}\n",
        "\n",
        "#grid = GridSearchCV(estimator=model_class, \n",
        "#                    param_grid=param_grid, \n",
        "#                    n_jobs=-1, \n",
        "#                    verbose=1)\n",
        "\n",
        "#grid_result = grid.fit(dtm[:1000], everyday_df['Rating'][:1000])\n",
        "\n",
        "#print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "#means = grid_result.cv_results_['mean_test_score']\n",
        "#stds = grid_result.cv_results_['std_test_score']\n",
        "#params = grid_result.cv_results_['params']\n",
        "\n",
        "#for mean, stdev, param in zip(means, stds, params):\n",
        "#    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.0346 - accuracy: 0.0350 - val_loss: 2.7295 - val_accuracy: 0.0500\n",
            "Epoch 2/7\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 2.3448 - accuracy: 0.0410 - val_loss: 1.8229 - val_accuracy: 0.0500\n",
            "Epoch 3/7\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 1.4151 - accuracy: 0.0410 - val_loss: 0.9690 - val_accuracy: 0.0500\n",
            "Epoch 4/7\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.8561 - accuracy: 0.0410 - val_loss: 0.6821 - val_accuracy: 0.0500\n",
            "Epoch 5/7\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.7037 - accuracy: 0.0410 - val_loss: 0.6128 - val_accuracy: 0.0500\n",
            "Epoch 6/7\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6457 - accuracy: 0.0410 - val_loss: 0.5857 - val_accuracy: 0.0500\n",
            "Epoch 7/7\n",
            "32/32 [==============================] - 0s 3ms/step - loss: 0.6079 - accuracy: 0.0410 - val_loss: 0.5661 - val_accuracy: 0.0500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5jnUb1GDglS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}