{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "u50QLBDD1ORj"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "everyday_df = pd.read_csv('/content/Cannabis_Strains_Features.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "Ny3VcyZn189Y",
        "outputId": "12406b3e-7018-44ec-fd65-24f6b04f9f17"
      },
      "source": [
        "everyday_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Strain</th>\n",
              "      <th>Type</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Effects</th>\n",
              "      <th>Flavor</th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100-Og</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Creative,Energetic,Tingly,Euphoric,Relaxed</td>\n",
              "      <td>Earthy,Sweet,Citrus</td>\n",
              "      <td>$100 OG is a 50/50 hybrid strain that packs a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>98-White-Widow</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.7</td>\n",
              "      <td>Relaxed,Aroused,Creative,Happy,Energetic</td>\n",
              "      <td>Flowery,Violet,Diesel</td>\n",
              "      <td>The ‘98 Aloha White Widow is an especially pot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1024</td>\n",
              "      <td>sativa</td>\n",
              "      <td>4.4</td>\n",
              "      <td>Uplifted,Happy,Relaxed,Energetic,Creative</td>\n",
              "      <td>Spicy/Herbal,Sage,Woody</td>\n",
              "      <td>1024 is a sativa-dominant hybrid bred in Spain...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13-Dawgs</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.2</td>\n",
              "      <td>Tingly,Creative,Hungry,Relaxed,Uplifted</td>\n",
              "      <td>Apricot,Citrus,Grapefruit</td>\n",
              "      <td>13 Dawgs is a hybrid of G13 and Chemdawg genet...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>24K-Gold</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.6</td>\n",
              "      <td>Happy,Relaxed,Euphoric,Uplifted,Talkative</td>\n",
              "      <td>Citrus,Earthy,Orange</td>\n",
              "      <td>Also known as Kosher Tangie, 24k Gold is a 60%...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Strain  ...                                        Description\n",
              "0          100-Og  ...  $100 OG is a 50/50 hybrid strain that packs a ...\n",
              "1  98-White-Widow  ...  The ‘98 Aloha White Widow is an especially pot...\n",
              "2            1024  ...  1024 is a sativa-dominant hybrid bred in Spain...\n",
              "3        13-Dawgs  ...  13 Dawgs is a hybrid of G13 and Chemdawg genet...\n",
              "4        24K-Gold  ...  Also known as Kosher Tangie, 24k Gold is a 60%...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CoeluOIBHZ8",
        "outputId": "cf13b3d8-653d-49db-938b-fc0fd98704d2"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_lg==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz#egg=en_core_web_lg==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (50.3.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVzqT5vpEQRz"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_lg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87ptsrveNlPK"
      },
      "source": [
        "def tokenize(doc):\n",
        "  lemmas = []\n",
        "    \n",
        "  doc = nlp(doc)\n",
        "    \n",
        "  for token in doc: \n",
        "    if ((token.is_stop == False) and (token.is_punct == False) and (token.pos_ != 'PRON')):\n",
        "      lemmas.append(token.lemma_)\n",
        "  \n",
        "  return lemmas\n",
        "\n",
        "everyday_df['Description_2'] = everyday_df['Description'].apply(tokenize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzyHHCWBPRAE",
        "outputId": "e2de249d-a0e5-4d62-b005-fa35f646f58f"
      },
      "source": [
        "everyday_df['bow_col'] = \"string\"\n",
        "for i in range(0, len(everyday_df['Effects'])):\n",
        "  everyday_df['bow_col'][i] = everyday_df['Effects'][i].split(\",\") + everyday_df['Flavor'][i].split(\",\") + everyday_df['Description_2'][i]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "giVphhXkPkOK",
        "outputId": "47964704-245a-4552-8258-e35cbabdf71f"
      },
      "source": [
        "everyday_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Strain</th>\n",
              "      <th>Type</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Effects</th>\n",
              "      <th>Flavor</th>\n",
              "      <th>Description</th>\n",
              "      <th>Description_2</th>\n",
              "      <th>bow_col</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100-Og</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Creative,Energetic,Tingly,Euphoric,Relaxed</td>\n",
              "      <td>Earthy,Sweet,Citrus</td>\n",
              "      <td>$100 OG is a 50/50 hybrid strain that packs a ...</td>\n",
              "      <td>[$, 100, OG, 50/50, hybrid, strain, pack, stro...</td>\n",
              "      <td>[Creative, Energetic, Tingly, Euphoric, Relaxe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>98-White-Widow</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.7</td>\n",
              "      <td>Relaxed,Aroused,Creative,Happy,Energetic</td>\n",
              "      <td>Flowery,Violet,Diesel</td>\n",
              "      <td>The ‘98 Aloha White Widow is an especially pot...</td>\n",
              "      <td>[98, Aloha, White, Widow, especially, potent, ...</td>\n",
              "      <td>[Relaxed, Aroused, Creative, Happy, Energetic,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1024</td>\n",
              "      <td>sativa</td>\n",
              "      <td>4.4</td>\n",
              "      <td>Uplifted,Happy,Relaxed,Energetic,Creative</td>\n",
              "      <td>Spicy/Herbal,Sage,Woody</td>\n",
              "      <td>1024 is a sativa-dominant hybrid bred in Spain...</td>\n",
              "      <td>[1024, sativa, dominant, hybrid, breed, Spain,...</td>\n",
              "      <td>[Uplifted, Happy, Relaxed, Energetic, Creative...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13-Dawgs</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.2</td>\n",
              "      <td>Tingly,Creative,Hungry,Relaxed,Uplifted</td>\n",
              "      <td>Apricot,Citrus,Grapefruit</td>\n",
              "      <td>13 Dawgs is a hybrid of G13 and Chemdawg genet...</td>\n",
              "      <td>[13, Dawgs, hybrid, G13, Chemdawg, genetic, br...</td>\n",
              "      <td>[Tingly, Creative, Hungry, Relaxed, Uplifted, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>24K-Gold</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.6</td>\n",
              "      <td>Happy,Relaxed,Euphoric,Uplifted,Talkative</td>\n",
              "      <td>Citrus,Earthy,Orange</td>\n",
              "      <td>Also known as Kosher Tangie, 24k Gold is a 60%...</td>\n",
              "      <td>[know, Kosher, Tangie, 24k, gold, 60, indica, ...</td>\n",
              "      <td>[Happy, Relaxed, Euphoric, Uplifted, Talkative...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Strain  ...                                            bow_col\n",
              "0          100-Og  ...  [Creative, Energetic, Tingly, Euphoric, Relaxe...\n",
              "1  98-White-Widow  ...  [Relaxed, Aroused, Creative, Happy, Energetic,...\n",
              "2            1024  ...  [Uplifted, Happy, Relaxed, Energetic, Creative...\n",
              "3        13-Dawgs  ...  [Tingly, Creative, Hungry, Relaxed, Uplifted, ...\n",
              "4        24K-Gold  ...  [Happy, Relaxed, Euphoric, Uplifted, Talkative...\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrvdOg4_P0N5",
        "outputId": "7db8293e-d697-470d-ec73-c17d8a42a781"
      },
      "source": [
        "everyday_df['bow_col'][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Creative',\n",
              " 'Energetic',\n",
              " 'Tingly',\n",
              " 'Euphoric',\n",
              " 'Relaxed',\n",
              " 'Earthy',\n",
              " 'Sweet',\n",
              " 'Citrus',\n",
              " '$',\n",
              " '100',\n",
              " 'OG',\n",
              " '50/50',\n",
              " 'hybrid',\n",
              " 'strain',\n",
              " 'pack',\n",
              " 'strong',\n",
              " 'punch',\n",
              " 'supposedly',\n",
              " 'refer',\n",
              " 'strength',\n",
              " 'high',\n",
              " 'price',\n",
              " 'start',\n",
              " 'show',\n",
              " 'Hollywood',\n",
              " 'plant',\n",
              " '$',\n",
              " '100',\n",
              " 'OG',\n",
              " 'tend',\n",
              " 'produce',\n",
              " 'large',\n",
              " 'dark',\n",
              " 'green',\n",
              " 'bud',\n",
              " 'stem',\n",
              " 'user',\n",
              " 'report',\n",
              " 'strong',\n",
              " 'body',\n",
              " 'effect',\n",
              " 'indica',\n",
              " 'pain',\n",
              " 'relief',\n",
              " 'alert',\n",
              " 'cerebral',\n",
              " 'feeling',\n",
              " 'thank',\n",
              " 'sativa']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuHrKuszQ6Zx",
        "outputId": "3df027ba-3d34-4ea2-a384-1fad3d3845c0"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "word_counts = Counter()\n",
        "everyday_df['bow_col'].apply(lambda x: word_counts.update(x))\n",
        "word_counts.most_common(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('strain', 3021),\n",
              " ('  ', 2138),\n",
              " ('Happy', 1902),\n",
              " ('Relaxed', 1758),\n",
              " ('Euphoric', 1669),\n",
              " ('effect', 1596),\n",
              " ('Uplifted', 1535),\n",
              " ('indica', 1399),\n",
              " ('hybrid', 1312),\n",
              " ('Sweet', 1155)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "OC2PRHNBQkt-",
        "outputId": "24651b29-28ae-4f5b-8241-7fc3701f930e"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vect = TfidfVectorizer(stop_words=nlp.Defaults.stop_words, max_features=1000)\n",
        "vect.fit(everyday_df['bow_col'].astype('str'))\n",
        "dtm = vect.transform(everyday_df['bow_col'].astype('str'))\n",
        "dtm = pd.DataFrame(dtm.todense(),columns=vect.get_feature_names())\n",
        "dtm.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>18</th>\n",
              "      <th>1st</th>\n",
              "      <th>20</th>\n",
              "      <th>2012</th>\n",
              "      <th>2014</th>\n",
              "      <th>2015</th>\n",
              "      <th>2016</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>2nd</th>\n",
              "      <th>30</th>\n",
              "      <th>3rd</th>\n",
              "      <th>40</th>\n",
              "      <th>47</th>\n",
              "      <th>50</th>\n",
              "      <th>55</th>\n",
              "      <th>60</th>\n",
              "      <th>65</th>\n",
              "      <th>70</th>\n",
              "      <th>75</th>\n",
              "      <th>80</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>abate</th>\n",
              "      <th>ability</th>\n",
              "      <th>accent</th>\n",
              "      <th>ace</th>\n",
              "      <th>ache</th>\n",
              "      <th>achieve</th>\n",
              "      <th>act</th>\n",
              "      <th>active</th>\n",
              "      <th>activity</th>\n",
              "      <th>...</th>\n",
              "      <th>useful</th>\n",
              "      <th>user</th>\n",
              "      <th>usher</th>\n",
              "      <th>usually</th>\n",
              "      <th>utilize</th>\n",
              "      <th>valley</th>\n",
              "      <th>vanilla</th>\n",
              "      <th>variation</th>\n",
              "      <th>variety</th>\n",
              "      <th>version</th>\n",
              "      <th>veteran</th>\n",
              "      <th>vibrant</th>\n",
              "      <th>vigorous</th>\n",
              "      <th>violet</th>\n",
              "      <th>wait</th>\n",
              "      <th>want</th>\n",
              "      <th>warm</th>\n",
              "      <th>washington</th>\n",
              "      <th>way</th>\n",
              "      <th>week</th>\n",
              "      <th>weigh</th>\n",
              "      <th>weight</th>\n",
              "      <th>weighted</th>\n",
              "      <th>west</th>\n",
              "      <th>white</th>\n",
              "      <th>wide</th>\n",
              "      <th>widow</th>\n",
              "      <th>win</th>\n",
              "      <th>winner</th>\n",
              "      <th>wonder</th>\n",
              "      <th>woody</th>\n",
              "      <th>work</th>\n",
              "      <th>world</th>\n",
              "      <th>worth</th>\n",
              "      <th>wrap</th>\n",
              "      <th>wreck</th>\n",
              "      <th>xa0</th>\n",
              "      <th>year</th>\n",
              "      <th>yield</th>\n",
              "      <th>zesty</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.347529</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.173267</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.122812</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.269103</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.151957</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.353412</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.450098</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.117527</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.620051</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.115618</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.154098</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.151959</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.116128</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.121769</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1000 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    10   11   12        13   14   15  ...  wrap  wreck  xa0  year  yield  zesty\n",
              "0  0.0  0.0  0.0  0.000000  0.0  0.0  ...   0.0    0.0  0.0   0.0    0.0    0.0\n",
              "1  0.0  0.0  0.0  0.000000  0.0  0.0  ...   0.0    0.0  0.0   0.0    0.0    0.0\n",
              "2  0.0  0.0  0.0  0.000000  0.0  0.0  ...   0.0    0.0  0.0   0.0    0.0    0.0\n",
              "3  0.0  0.0  0.0  0.620051  0.0  0.0  ...   0.0    0.0  0.0   0.0    0.0    0.0\n",
              "4  0.0  0.0  0.0  0.000000  0.0  0.0  ...   0.0    0.0  0.0   0.0    0.0    0.0\n",
              "\n",
              "[5 rows x 1000 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7sQJJ0zR3Ey",
        "outputId": "095eb7f6-56c6-4c64-9cf4-6b255f65b97b"
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "nn = NearestNeighbors(n_neighbors=10, algorithm='kd_tree')\n",
        "nn.fit(dtm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NearestNeighbors(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n",
              "                 metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
              "                 radius=1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gDSvLDovRN2"
      },
      "source": [
        "random_pot = [\"I want something to take the edge off of my day, really mellow me out and help get me in the zone\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5U2ZiOk3R89z"
      },
      "source": [
        "new = vect.transform(random_pot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK2MlRBpvmNx",
        "outputId": "40101a81-53a9-4867-8b0c-5cbff24a3ae7"
      },
      "source": [
        "nn.kneighbors(new.todense())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1.        , 1.        , 1.        , 1.21833403, 1.22405446,\n",
              "         1.25974847, 1.27575554, 1.27670641, 1.28326677, 1.29198692]]),\n",
              " array([[1652, 1653, 1651, 2318, 1785,  672, 2251,  166, 2068, 1124]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5x-DdjwVwmZf",
        "outputId": "01e9eab4-da9e-44c7-c202-17b53dc95ebc"
      },
      "source": [
        "everyday_df['bow_col'][2068]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Relaxed',\n",
              " 'Focused',\n",
              " 'Uplifted',\n",
              " 'Talkative',\n",
              " 'Happy',\n",
              " 'Pine',\n",
              " 'Earthy',\n",
              " 'Spicy/Herbal',\n",
              " 'Suzy',\n",
              " 'Q',\n",
              " 'high',\n",
              " 'CBD',\n",
              " 'low',\n",
              " 'thc',\n",
              " 'strain',\n",
              " 'piney',\n",
              " 'taste',\n",
              " 'help',\n",
              " 'treat',\n",
              " 'symptom',\n",
              " 'little',\n",
              " 'high',\n",
              " 'great',\n",
              " 'strain',\n",
              " 'add',\n",
              " 'burgeon',\n",
              " 'world',\n",
              " 'CBD',\n",
              " 'strain',\n",
              " 'strain',\n",
              " 'test',\n",
              " 'upwards',\n",
              " '59:1',\n",
              " 'CBD',\n",
              " 'THC',\n",
              " 'hybrid',\n",
              " 'great',\n",
              " 'daytime',\n",
              " 'use',\n",
              " 'want',\n",
              " 'relieve',\n",
              " 'chronic',\n",
              " 'pain',\n",
              " 'nausea',\n",
              " 'arthritis',\n",
              " 'muscle',\n",
              " 'spasm',\n",
              " 'anxiety',\n",
              " 'psychoactive',\n",
              " 'effect',\n",
              " '\\xa0 \\xa0\\xa0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQawZ0X61nNO",
        "outputId": "ae99c325-bd1e-472e-863e-6a80ecde2ef7"
      },
      "source": [
        "everyday_df['Rating'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.5    323\n",
              "4.3    299\n",
              "4.4    287\n",
              "4.6    245\n",
              "4.2    227\n",
              "5.0    219\n",
              "4.7    166\n",
              "4.8    162\n",
              "4.0    109\n",
              "4.1    101\n",
              "4.9     61\n",
              "0.0     35\n",
              "3.9     29\n",
              "3.8     23\n",
              "3.7     16\n",
              "3.0     13\n",
              "3.6     11\n",
              "3.5     10\n",
              "3.4      5\n",
              "3.2      3\n",
              "3.3      3\n",
              "3.1      1\n",
              "2.5      1\n",
              "1.0      1\n",
              "2.8      1\n",
              "Name: Rating, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo_LhDrYxoD4",
        "outputId": "9381ad02-5cb1-44f3-e4f9-d3a6b1da6db0"
      },
      "source": [
        "import numpy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "def create_model(units=32):\n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Dense(units, activation=\"relu\", input_dim=1000))\n",
        "  \n",
        "  n_nodes_output = 25\n",
        "  model.add(Dense(n_nodes_output, activation='softmax'))\n",
        "  \n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "#results = model.fit(dtm[:1000], \n",
        "#                    everyday_df['Rating'][:1000],\n",
        "#                    epochs=5,\n",
        "#                    batch_size=4,\n",
        "#                    validation_data=(dtm[1000:2000], everyday_df['Rating'][1000:2000]))\n",
        "\n",
        "model_class = KerasClassifier(build_fn=create_model, verbose=1)\n",
        "\n",
        "param_grid = {'batch_size': [4, 8],\n",
        "              'epochs': [5, 6, 9],\n",
        "              'units':[24, 32]}\n",
        "\n",
        "grid = GridSearchCV(estimator=model_class, \n",
        "                    param_grid=param_grid, \n",
        "                    n_jobs=1, \n",
        "                    verbose=1)\n",
        "\n",
        "grid_result = grid.fit(dtm[:1000], everyday_df['Rating'][:1000])\n",
        "\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "200/200 [==============================] - 0s 1ms/step - loss: 3.0141 - accuracy: 0.1100\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5315 - accuracy: 0.1925\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3712 - accuracy: 0.3088\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2438 - accuracy: 0.3288\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1045 - accuracy: 0.4538\n",
            "50/50 [==============================] - 0s 832us/step - loss: 2.5118 - accuracy: 0.2350\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9559 - accuracy: 0.1125\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4965 - accuracy: 0.1350\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3731 - accuracy: 0.3000\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2633 - accuracy: 0.3938\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1371 - accuracy: 0.4550\n",
            "50/50 [==============================] - 0s 801us/step - loss: 2.5894 - accuracy: 0.1750\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 3.0009 - accuracy: 0.1100\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5879 - accuracy: 0.1450\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4668 - accuracy: 0.2763\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3595 - accuracy: 0.3425\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2222 - accuracy: 0.3750\n",
            "50/50 [==============================] - 0s 798us/step - loss: 2.3469 - accuracy: 0.2350\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9737 - accuracy: 0.1150\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5261 - accuracy: 0.1912\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3857 - accuracy: 0.2875\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2609 - accuracy: 0.3537\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1229 - accuracy: 0.4250\n",
            "50/50 [==============================] - 0s 814us/step - loss: 2.5266 - accuracy: 0.1800\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 3.0396 - accuracy: 0.1088\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5756 - accuracy: 0.1750\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4360 - accuracy: 0.2775\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3107 - accuracy: 0.3600\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1635 - accuracy: 0.4150\n",
            "50/50 [==============================] - 0s 880us/step - loss: 2.4004 - accuracy: 0.2400\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 3.0046 - accuracy: 0.1025\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5067 - accuracy: 0.1850\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3474 - accuracy: 0.3025\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1922 - accuracy: 0.4375\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0209 - accuracy: 0.4500\n",
            "50/50 [==============================] - 0s 792us/step - loss: 2.5084 - accuracy: 0.2150\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9690 - accuracy: 0.1225\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4930 - accuracy: 0.1663\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3366 - accuracy: 0.3625\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1815 - accuracy: 0.4675\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0062 - accuracy: 0.5088\n",
            "50/50 [==============================] - 0s 814us/step - loss: 2.5975 - accuracy: 0.1750\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9773 - accuracy: 0.1200\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5397 - accuracy: 0.1850\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4045 - accuracy: 0.2862\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2662 - accuracy: 0.3625\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1073 - accuracy: 0.4100\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2.3198 - accuracy: 0.2250\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9911 - accuracy: 0.1100\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5198 - accuracy: 0.2013\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3691 - accuracy: 0.2988\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2123 - accuracy: 0.4087\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0315 - accuracy: 0.4875\n",
            "50/50 [==============================] - 0s 834us/step - loss: 2.5377 - accuracy: 0.1700\n",
            "Epoch 1/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9670 - accuracy: 0.1213\n",
            "Epoch 2/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5263 - accuracy: 0.1600\n",
            "Epoch 3/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3788 - accuracy: 0.3225\n",
            "Epoch 4/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2264 - accuracy: 0.4038\n",
            "Epoch 5/5\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0521 - accuracy: 0.4975\n",
            "50/50 [==============================] - 0s 791us/step - loss: 2.3926 - accuracy: 0.2200\n",
            "Epoch 1/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9661 - accuracy: 0.1112\n",
            "Epoch 2/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5130 - accuracy: 0.1475\n",
            "Epoch 3/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3863 - accuracy: 0.3038\n",
            "Epoch 4/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2712 - accuracy: 0.3800\n",
            "Epoch 5/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1362 - accuracy: 0.4400\n",
            "Epoch 6/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.9864 - accuracy: 0.4800\n",
            "50/50 [==============================] - 0s 871us/step - loss: 2.5392 - accuracy: 0.2200\n",
            "Epoch 1/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9971 - accuracy: 0.1262\n",
            "Epoch 2/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5141 - accuracy: 0.1950\n",
            "Epoch 3/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3873 - accuracy: 0.2763\n",
            "Epoch 4/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2720 - accuracy: 0.3288\n",
            "Epoch 5/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1352 - accuracy: 0.4725\n",
            "Epoch 6/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.9833 - accuracy: 0.5025\n",
            "50/50 [==============================] - 0s 979us/step - loss: 2.5967 - accuracy: 0.1500\n",
            "Epoch 1/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9915 - accuracy: 0.1200\n",
            "Epoch 2/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5534 - accuracy: 0.1663\n",
            "Epoch 3/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4203 - accuracy: 0.2862\n",
            "Epoch 4/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3013 - accuracy: 0.3663\n",
            "Epoch 5/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1716 - accuracy: 0.3988\n",
            "Epoch 6/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0311 - accuracy: 0.4575\n",
            "50/50 [==============================] - 0s 883us/step - loss: 2.3355 - accuracy: 0.2550\n",
            "Epoch 1/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9555 - accuracy: 0.1187\n",
            "Epoch 2/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5067 - accuracy: 0.2188\n",
            "Epoch 3/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3926 - accuracy: 0.2713\n",
            "Epoch 4/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2833 - accuracy: 0.3438\n",
            "Epoch 5/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1543 - accuracy: 0.4087\n",
            "Epoch 6/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0129 - accuracy: 0.4512\n",
            "50/50 [==============================] - 0s 875us/step - loss: 2.5240 - accuracy: 0.1650\n",
            "Epoch 1/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9545 - accuracy: 0.1013\n",
            "Epoch 2/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5405 - accuracy: 0.1637\n",
            "Epoch 3/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4138 - accuracy: 0.2850\n",
            "Epoch 4/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2944 - accuracy: 0.3425\n",
            "Epoch 5/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1559 - accuracy: 0.4025\n",
            "Epoch 6/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0081 - accuracy: 0.4650\n",
            "50/50 [==============================] - 0s 850us/step - loss: 2.3959 - accuracy: 0.2100\n",
            "Epoch 1/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 3.0291 - accuracy: 0.1112\n",
            "Epoch 2/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5256 - accuracy: 0.1950\n",
            "Epoch 3/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3651 - accuracy: 0.2800\n",
            "Epoch 4/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2157 - accuracy: 0.4150\n",
            "Epoch 5/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0370 - accuracy: 0.4712\n",
            "Epoch 6/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.8434 - accuracy: 0.5362\n",
            "50/50 [==============================] - 0s 864us/step - loss: 2.5028 - accuracy: 0.2050\n",
            "Epoch 1/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9440 - accuracy: 0.1238\n",
            "Epoch 2/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4797 - accuracy: 0.2100\n",
            "Epoch 3/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3398 - accuracy: 0.3212\n",
            "Epoch 4/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1833 - accuracy: 0.4050\n",
            "Epoch 5/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0009 - accuracy: 0.4812\n",
            "Epoch 6/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.8073 - accuracy: 0.5700\n",
            "50/50 [==============================] - 0s 964us/step - loss: 2.6055 - accuracy: 0.1400\n",
            "Epoch 1/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9811 - accuracy: 0.1050\n",
            "Epoch 2/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5460 - accuracy: 0.1900\n",
            "Epoch 3/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4202 - accuracy: 0.2837\n",
            "Epoch 4/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2840 - accuracy: 0.3762\n",
            "Epoch 5/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1225 - accuracy: 0.4137\n",
            "Epoch 6/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.9449 - accuracy: 0.4950\n",
            "50/50 [==============================] - 0s 812us/step - loss: 2.3107 - accuracy: 0.2650\n",
            "Epoch 1/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9791 - accuracy: 0.1262\n",
            "Epoch 2/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4869 - accuracy: 0.1775\n",
            "Epoch 3/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3410 - accuracy: 0.3413\n",
            "Epoch 4/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1957 - accuracy: 0.4175\n",
            "Epoch 5/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0277 - accuracy: 0.4925\n",
            "Epoch 6/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.8502 - accuracy: 0.5462\n",
            "50/50 [==============================] - 0s 818us/step - loss: 2.5241 - accuracy: 0.1850\n",
            "Epoch 1/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9504 - accuracy: 0.1075\n",
            "Epoch 2/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5119 - accuracy: 0.2113\n",
            "Epoch 3/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3714 - accuracy: 0.3288\n",
            "Epoch 4/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2242 - accuracy: 0.4025\n",
            "Epoch 5/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0570 - accuracy: 0.4825\n",
            "Epoch 6/6\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.8778 - accuracy: 0.5138\n",
            "50/50 [==============================] - 0s 824us/step - loss: 2.3914 - accuracy: 0.2150\n",
            "Epoch 1/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 3.0182 - accuracy: 0.1187\n",
            "Epoch 2/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5588 - accuracy: 0.1925\n",
            "Epoch 3/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4245 - accuracy: 0.2275\n",
            "Epoch 4/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3035 - accuracy: 0.3050\n",
            "Epoch 5/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1527 - accuracy: 0.4450\n",
            "Epoch 6/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.9805 - accuracy: 0.4888\n",
            "Epoch 7/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.8059 - accuracy: 0.5788\n",
            "Epoch 8/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.6319 - accuracy: 0.6250\n",
            "Epoch 9/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.4617 - accuracy: 0.6975\n",
            "50/50 [==============================] - 0s 831us/step - loss: 2.5314 - accuracy: 0.1950\n",
            "Epoch 1/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 3.0419 - accuracy: 0.1075\n",
            "Epoch 2/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5413 - accuracy: 0.1575\n",
            "Epoch 3/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3870 - accuracy: 0.2850\n",
            "Epoch 4/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2563 - accuracy: 0.4275\n",
            "Epoch 5/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1100 - accuracy: 0.4512\n",
            "Epoch 6/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.9529 - accuracy: 0.5200\n",
            "Epoch 7/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.7899 - accuracy: 0.5950\n",
            "Epoch 8/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.6342 - accuracy: 0.6325\n",
            "Epoch 9/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.4801 - accuracy: 0.7038\n",
            "50/50 [==============================] - 0s 896us/step - loss: 2.6594 - accuracy: 0.1400\n",
            "Epoch 1/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 3.0360 - accuracy: 0.1063\n",
            "Epoch 2/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5843 - accuracy: 0.1575\n",
            "Epoch 3/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4403 - accuracy: 0.2738\n",
            "Epoch 4/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3259 - accuracy: 0.3162\n",
            "Epoch 5/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1925 - accuracy: 0.4412\n",
            "Epoch 6/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0468 - accuracy: 0.4988\n",
            "Epoch 7/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.8936 - accuracy: 0.5587\n",
            "Epoch 8/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.7360 - accuracy: 0.6237\n",
            "Epoch 9/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.5770 - accuracy: 0.6687\n",
            "50/50 [==============================] - 0s 839us/step - loss: 2.3167 - accuracy: 0.2300\n",
            "Epoch 1/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9868 - accuracy: 0.1225\n",
            "Epoch 2/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5176 - accuracy: 0.1500\n",
            "Epoch 3/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3741 - accuracy: 0.2725\n",
            "Epoch 4/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2597 - accuracy: 0.3725\n",
            "Epoch 5/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1315 - accuracy: 0.4225\n",
            "Epoch 6/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.9888 - accuracy: 0.4900\n",
            "Epoch 7/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.8400 - accuracy: 0.5288\n",
            "Epoch 8/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.6827 - accuracy: 0.6212\n",
            "Epoch 9/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.5262 - accuracy: 0.6775\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2.5084 - accuracy: 0.1750\n",
            "Epoch 1/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 3.0091 - accuracy: 0.1088\n",
            "Epoch 2/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5620 - accuracy: 0.1650\n",
            "Epoch 3/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4215 - accuracy: 0.2637\n",
            "Epoch 4/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2958 - accuracy: 0.3675\n",
            "Epoch 5/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1569 - accuracy: 0.4525\n",
            "Epoch 6/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0031 - accuracy: 0.5138\n",
            "Epoch 7/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.8478 - accuracy: 0.5450\n",
            "Epoch 8/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.6892 - accuracy: 0.5975\n",
            "Epoch 9/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.5377 - accuracy: 0.6525\n",
            "50/50 [==============================] - 0s 945us/step - loss: 2.3692 - accuracy: 0.2250\n",
            "Epoch 1/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9636 - accuracy: 0.1287\n",
            "Epoch 2/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4914 - accuracy: 0.2175\n",
            "Epoch 3/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3474 - accuracy: 0.3225\n",
            "Epoch 4/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1990 - accuracy: 0.4325\n",
            "Epoch 5/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0263 - accuracy: 0.4737\n",
            "Epoch 6/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.8365 - accuracy: 0.5412\n",
            "Epoch 7/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.6472 - accuracy: 0.6087\n",
            "Epoch 8/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.4617 - accuracy: 0.6913\n",
            "Epoch 9/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.2894 - accuracy: 0.7275\n",
            "50/50 [==============================] - 0s 928us/step - loss: 2.5726 - accuracy: 0.1750\n",
            "Epoch 1/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9277 - accuracy: 0.1238\n",
            "Epoch 2/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4750 - accuracy: 0.1750\n",
            "Epoch 3/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3243 - accuracy: 0.3125\n",
            "Epoch 4/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1742 - accuracy: 0.4475\n",
            "Epoch 5/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0024 - accuracy: 0.4688\n",
            "Epoch 6/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.8169 - accuracy: 0.5688\n",
            "Epoch 7/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.6292 - accuracy: 0.6237\n",
            "Epoch 8/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.4444 - accuracy: 0.6837\n",
            "Epoch 9/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.2713 - accuracy: 0.7337\n",
            "50/50 [==============================] - 0s 829us/step - loss: 2.7284 - accuracy: 0.1400\n",
            "Epoch 1/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9558 - accuracy: 0.1100\n",
            "Epoch 2/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5456 - accuracy: 0.1787\n",
            "Epoch 3/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4156 - accuracy: 0.2788\n",
            "Epoch 4/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2696 - accuracy: 0.3187\n",
            "Epoch 5/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1025 - accuracy: 0.4025\n",
            "Epoch 6/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.9193 - accuracy: 0.4913\n",
            "Epoch 7/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.7366 - accuracy: 0.6087\n",
            "Epoch 8/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.5485 - accuracy: 0.6825\n",
            "Epoch 9/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.3688 - accuracy: 0.7462\n",
            "50/50 [==============================] - 0s 825us/step - loss: 2.2964 - accuracy: 0.2350\n",
            "Epoch 1/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.9645 - accuracy: 0.1312\n",
            "Epoch 2/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.4827 - accuracy: 0.2150\n",
            "Epoch 3/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3433 - accuracy: 0.3313\n",
            "Epoch 4/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.1936 - accuracy: 0.3750\n",
            "Epoch 5/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0269 - accuracy: 0.4238\n",
            "Epoch 6/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.8563 - accuracy: 0.4825\n",
            "Epoch 7/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.6858 - accuracy: 0.5725\n",
            "Epoch 8/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.5167 - accuracy: 0.6225\n",
            "Epoch 9/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.3565 - accuracy: 0.6975\n",
            "50/50 [==============================] - 0s 900us/step - loss: 2.5589 - accuracy: 0.1750\n",
            "Epoch 1/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 3.0054 - accuracy: 0.1112\n",
            "Epoch 2/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.5282 - accuracy: 0.2262\n",
            "Epoch 3/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.3752 - accuracy: 0.3162\n",
            "Epoch 4/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.2241 - accuracy: 0.4038\n",
            "Epoch 5/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 2.0478 - accuracy: 0.4600\n",
            "Epoch 6/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.8607 - accuracy: 0.5487\n",
            "Epoch 7/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.6727 - accuracy: 0.6288\n",
            "Epoch 8/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.4834 - accuracy: 0.6913\n",
            "Epoch 9/9\n",
            "200/200 [==============================] - 0s 1ms/step - loss: 1.3067 - accuracy: 0.7400\n",
            "50/50 [==============================] - 0s 1ms/step - loss: 2.4278 - accuracy: 0.2050\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1422 - accuracy: 0.0825\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7713 - accuracy: 0.1213\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4903 - accuracy: 0.2463\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3802 - accuracy: 0.2875\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2867 - accuracy: 0.3400\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5403 - accuracy: 0.2250\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1029 - accuracy: 0.1175\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6864 - accuracy: 0.1275\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4505 - accuracy: 0.1775\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3551 - accuracy: 0.3262\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2737 - accuracy: 0.3837\n",
            "25/25 [==============================] - 0s 974us/step - loss: 2.6149 - accuracy: 0.1900\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0978 - accuracy: 0.1013\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7009 - accuracy: 0.1488\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.5146 - accuracy: 0.2350\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4302 - accuracy: 0.3225\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3511 - accuracy: 0.3537\n",
            "25/25 [==============================] - 0s 1000us/step - loss: 2.3864 - accuracy: 0.2500\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1436 - accuracy: 0.1125\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7826 - accuracy: 0.1775\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4943 - accuracy: 0.2037\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3780 - accuracy: 0.2862\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2807 - accuracy: 0.3413\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5537 - accuracy: 0.1950\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1173 - accuracy: 0.0775\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7598 - accuracy: 0.1538\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.5333 - accuracy: 0.2163\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4278 - accuracy: 0.3013\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3330 - accuracy: 0.3562\n",
            "25/25 [==============================] - 0s 1000us/step - loss: 2.4444 - accuracy: 0.1800\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0778 - accuracy: 0.1013\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6557 - accuracy: 0.1625\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4552 - accuracy: 0.1813\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3431 - accuracy: 0.3413\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2264 - accuracy: 0.4187\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5473 - accuracy: 0.2000\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1292 - accuracy: 0.1112\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7101 - accuracy: 0.1587\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4479 - accuracy: 0.2512\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3284 - accuracy: 0.4238\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2161 - accuracy: 0.4112\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.6019 - accuracy: 0.1850\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0551 - accuracy: 0.1287\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6359 - accuracy: 0.2062\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4615 - accuracy: 0.2925\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3595 - accuracy: 0.3500\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2524 - accuracy: 0.3625\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.3421 - accuracy: 0.2500\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0760 - accuracy: 0.1037\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6160 - accuracy: 0.1725\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4234 - accuracy: 0.2738\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3156 - accuracy: 0.3638\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2021 - accuracy: 0.4238\n",
            "25/25 [==============================] - 0s 942us/step - loss: 2.5234 - accuracy: 0.1850\n",
            "Epoch 1/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1077 - accuracy: 0.1175\n",
            "Epoch 2/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7183 - accuracy: 0.1737\n",
            "Epoch 3/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.5138 - accuracy: 0.2125\n",
            "Epoch 4/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4074 - accuracy: 0.3175\n",
            "Epoch 5/5\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2905 - accuracy: 0.4187\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.4359 - accuracy: 0.2150\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1446 - accuracy: 0.0988\n",
            "Epoch 2/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.8147 - accuracy: 0.1587\n",
            "Epoch 3/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.5201 - accuracy: 0.2675\n",
            "Epoch 4/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3869 - accuracy: 0.3000\n",
            "Epoch 5/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2720 - accuracy: 0.3812\n",
            "Epoch 6/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1547 - accuracy: 0.4300\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5302 - accuracy: 0.2400\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1365 - accuracy: 0.0988\n",
            "Epoch 2/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7690 - accuracy: 0.1825\n",
            "Epoch 3/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4914 - accuracy: 0.2887\n",
            "Epoch 4/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3784 - accuracy: 0.3262\n",
            "Epoch 5/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2778 - accuracy: 0.4087\n",
            "Epoch 6/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1655 - accuracy: 0.4688\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.6135 - accuracy: 0.1850\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1073 - accuracy: 0.1037\n",
            "Epoch 2/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7195 - accuracy: 0.1238\n",
            "Epoch 3/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.5207 - accuracy: 0.1488\n",
            "Epoch 4/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4331 - accuracy: 0.3075\n",
            "Epoch 5/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3475 - accuracy: 0.3950\n",
            "Epoch 6/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2498 - accuracy: 0.4175\n",
            "25/25 [==============================] - 0s 948us/step - loss: 2.3572 - accuracy: 0.2400\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0896 - accuracy: 0.1225\n",
            "Epoch 2/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6866 - accuracy: 0.1688\n",
            "Epoch 3/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4809 - accuracy: 0.2350\n",
            "Epoch 4/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3836 - accuracy: 0.2975\n",
            "Epoch 5/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2911 - accuracy: 0.3650\n",
            "Epoch 6/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1902 - accuracy: 0.4175\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5418 - accuracy: 0.1650\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1297 - accuracy: 0.0988\n",
            "Epoch 2/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7603 - accuracy: 0.1363\n",
            "Epoch 3/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.5074 - accuracy: 0.1850\n",
            "Epoch 4/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4055 - accuracy: 0.3288\n",
            "Epoch 5/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3125 - accuracy: 0.3850\n",
            "Epoch 6/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2134 - accuracy: 0.4437\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.4135 - accuracy: 0.1900\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0920 - accuracy: 0.1075\n",
            "Epoch 2/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6599 - accuracy: 0.1525\n",
            "Epoch 3/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4418 - accuracy: 0.2237\n",
            "Epoch 4/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3252 - accuracy: 0.3663\n",
            "Epoch 5/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2081 - accuracy: 0.4263\n",
            "Epoch 6/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.0792 - accuracy: 0.4888\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5191 - accuracy: 0.2100\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0847 - accuracy: 0.1025\n",
            "Epoch 2/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6354 - accuracy: 0.1975\n",
            "Epoch 3/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4257 - accuracy: 0.3100\n",
            "Epoch 4/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3137 - accuracy: 0.3725\n",
            "Epoch 5/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2028 - accuracy: 0.4263\n",
            "Epoch 6/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.0828 - accuracy: 0.4675\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5852 - accuracy: 0.1650\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0697 - accuracy: 0.1025\n",
            "Epoch 2/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6433 - accuracy: 0.1612\n",
            "Epoch 3/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4783 - accuracy: 0.2612\n",
            "Epoch 4/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3748 - accuracy: 0.3537\n",
            "Epoch 5/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2674 - accuracy: 0.3825\n",
            "Epoch 6/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1514 - accuracy: 0.4275\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.3205 - accuracy: 0.2500\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0621 - accuracy: 0.1187\n",
            "Epoch 2/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6171 - accuracy: 0.1513\n",
            "Epoch 3/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4378 - accuracy: 0.2825\n",
            "Epoch 4/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3303 - accuracy: 0.3800\n",
            "Epoch 5/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2160 - accuracy: 0.4325\n",
            "Epoch 6/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.0918 - accuracy: 0.4900\n",
            " 1/25 [>.............................] - ETA: 0s - loss: 1.9364 - accuracy: 0.3750WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0019s vs `on_test_batch_end` time: 0.0041s). Check your callbacks.\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5387 - accuracy: 0.1750\n",
            "Epoch 1/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1312 - accuracy: 0.0900\n",
            "Epoch 2/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7198 - accuracy: 0.1475\n",
            "Epoch 3/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4832 - accuracy: 0.2288\n",
            "Epoch 4/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3789 - accuracy: 0.3350\n",
            "Epoch 5/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2767 - accuracy: 0.3688\n",
            "Epoch 6/6\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1584 - accuracy: 0.4375\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.4052 - accuracy: 0.1900\n",
            "Epoch 1/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1177 - accuracy: 0.1175\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7364 - accuracy: 0.1425\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4893 - accuracy: 0.1688\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3740 - accuracy: 0.2412\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2721 - accuracy: 0.3762\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1674 - accuracy: 0.3862\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.0589 - accuracy: 0.4450\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.9471 - accuracy: 0.5275\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.8314 - accuracy: 0.5688\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.4926 - accuracy: 0.2300\n",
            "Epoch 1/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0991 - accuracy: 0.1138\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6820 - accuracy: 0.1688\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4579 - accuracy: 0.2962\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3609 - accuracy: 0.3288\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2690 - accuracy: 0.4550\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1716 - accuracy: 0.5188\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.0664 - accuracy: 0.5763\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.9562 - accuracy: 0.5888\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.8417 - accuracy: 0.6200\n",
            "25/25 [==============================] - 0s 977us/step - loss: 2.6034 - accuracy: 0.1550\n",
            "Epoch 1/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0853 - accuracy: 0.1163\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6888 - accuracy: 0.1838\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.5061 - accuracy: 0.2700\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4234 - accuracy: 0.3187\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3411 - accuracy: 0.3887\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2527 - accuracy: 0.4300\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1556 - accuracy: 0.4725\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.0529 - accuracy: 0.5113\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.9430 - accuracy: 0.5888\n",
            "25/25 [==============================] - 0s 984us/step - loss: 2.3359 - accuracy: 0.2500\n",
            "Epoch 1/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1478 - accuracy: 0.1150\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.7863 - accuracy: 0.1700\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4994 - accuracy: 0.2387\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3916 - accuracy: 0.3525\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3023 - accuracy: 0.3625\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2059 - accuracy: 0.4563\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1007 - accuracy: 0.4688\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.9853 - accuracy: 0.5213\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.8717 - accuracy: 0.5550\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5175 - accuracy: 0.1750\n",
            "Epoch 1/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1464 - accuracy: 0.0925\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.8013 - accuracy: 0.1637\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.5352 - accuracy: 0.2375\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4189 - accuracy: 0.2912\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3223 - accuracy: 0.3963\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2162 - accuracy: 0.4112\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1045 - accuracy: 0.4737\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.9887 - accuracy: 0.5088\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.8682 - accuracy: 0.5387\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.3889 - accuracy: 0.2100\n",
            "Epoch 1/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0729 - accuracy: 0.1013\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6207 - accuracy: 0.1700\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4245 - accuracy: 0.2338\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3192 - accuracy: 0.3837\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2077 - accuracy: 0.3663\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.0893 - accuracy: 0.4863\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.9593 - accuracy: 0.5400\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.8280 - accuracy: 0.5763\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.6958 - accuracy: 0.6175\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5114 - accuracy: 0.2250\n",
            "Epoch 1/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.1065 - accuracy: 0.1138\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6483 - accuracy: 0.1625\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4173 - accuracy: 0.3300\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2972 - accuracy: 0.3550\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1787 - accuracy: 0.4437\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.0559 - accuracy: 0.4787\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.9275 - accuracy: 0.5325\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.7997 - accuracy: 0.5738\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.6674 - accuracy: 0.6062\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.6136 - accuracy: 0.1450\n",
            "Epoch 1/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0865 - accuracy: 0.1100\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6846 - accuracy: 0.1963\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4891 - accuracy: 0.2900\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3859 - accuracy: 0.3487\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2784 - accuracy: 0.3900\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1567 - accuracy: 0.4638\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.0281 - accuracy: 0.5100\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.8956 - accuracy: 0.5713\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.7605 - accuracy: 0.6087\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.3113 - accuracy: 0.2800\n",
            "Epoch 1/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0504 - accuracy: 0.1150\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6041 - accuracy: 0.1612\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4264 - accuracy: 0.2637\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3178 - accuracy: 0.3212\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2053 - accuracy: 0.3988\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.0889 - accuracy: 0.4650\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.9677 - accuracy: 0.5013\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.8422 - accuracy: 0.5400\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.7133 - accuracy: 0.6050\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.5039 - accuracy: 0.1400\n",
            "Epoch 1/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 3.0805 - accuracy: 0.1200\n",
            "Epoch 2/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.6442 - accuracy: 0.2037\n",
            "Epoch 3/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.4565 - accuracy: 0.2850\n",
            "Epoch 4/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.3488 - accuracy: 0.3750\n",
            "Epoch 5/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.2344 - accuracy: 0.4400\n",
            "Epoch 6/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 2.1110 - accuracy: 0.4963\n",
            "Epoch 7/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.9792 - accuracy: 0.5175\n",
            "Epoch 8/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.8445 - accuracy: 0.5675\n",
            "Epoch 9/9\n",
            "100/100 [==============================] - 0s 1ms/step - loss: 1.7107 - accuracy: 0.6125\n",
            "25/25 [==============================] - 0s 1ms/step - loss: 2.3967 - accuracy: 0.1950\n",
            "Epoch 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  2.0min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "250/250 [==============================] - 0s 1ms/step - loss: 2.9099 - accuracy: 0.1160\n",
            "Epoch 2/5\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 2.5002 - accuracy: 0.1790\n",
            "Epoch 3/5\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 2.3830 - accuracy: 0.2780\n",
            "Epoch 4/5\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 2.2568 - accuracy: 0.3950\n",
            "Epoch 5/5\n",
            "250/250 [==============================] - 0s 1ms/step - loss: 2.1127 - accuracy: 0.4400\n",
            "Best: 0.21299999952316284 using {'batch_size': 4, 'epochs': 5, 'units': 24}\n",
            "Means: 0.21299999952316284, Stdev: 0.029086077123937806 with: {'batch_size': 4, 'epochs': 5, 'units': 24}\n",
            "Means: 0.20099999904632568, Stdev: 0.02353720379671924 with: {'batch_size': 4, 'epochs': 5, 'units': 32}\n",
            "Means: 0.2, Stdev: 0.0380788609117004 with: {'batch_size': 4, 'epochs': 6, 'units': 24}\n",
            "Means: 0.20199999809265137, Stdev: 0.040693975290792335 with: {'batch_size': 4, 'epochs': 6, 'units': 32}\n",
            "Means: 0.1929999977350235, Stdev: 0.03325657812457448 with: {'batch_size': 4, 'epochs': 9, 'units': 24}\n",
            "Means: 0.18599999845027923, Stdev: 0.03199999984353784 with: {'batch_size': 4, 'epochs': 9, 'units': 32}\n",
            "Means: 0.20799999833106994, Stdev: 0.02580697451697236 with: {'batch_size': 8, 'epochs': 5, 'units': 24}\n",
            "Means: 0.2070000022649765, Stdev: 0.024207436071128498 with: {'batch_size': 8, 'epochs': 5, 'units': 32}\n",
            "Means: 0.20399999916553496, Stdev: 0.030561409300862423 with: {'batch_size': 8, 'epochs': 6, 'units': 24}\n",
            "Means: 0.1979999989271164, Stdev: 0.030099832508343853 with: {'batch_size': 8, 'epochs': 6, 'units': 32}\n",
            "Means: 0.20399999916553496, Stdev: 0.0348425033380531 with: {'batch_size': 8, 'epochs': 9, 'units': 24}\n",
            "Means: 0.19699999690055847, Stdev: 0.05221111042428463 with: {'batch_size': 8, 'epochs': 9, 'units': 32}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5jnUb1GDglS",
        "outputId": "62bbc0d5-f168-440c-b00f-1c9e703ea399"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model.save('eli_model.tf')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
            "INFO:tensorflow:Assets written to: eli_model.tf/assets\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}